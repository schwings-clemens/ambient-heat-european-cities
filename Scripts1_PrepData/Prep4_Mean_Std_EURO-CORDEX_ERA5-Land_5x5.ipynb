{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time as t_util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import cftime\n",
    "import xarray as xr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read main paths\n",
    "with open('../path_main.txt', 'r') as file:   path_main  = file.read()\n",
    "with open('../path_grids.txt', 'r') as file:  path_grids = file.read()\n",
    "\n",
    "dir_scripts    = f'{path_main}Scripts/'\n",
    "dir_CORDEX     = f'{path_main}Data/EURO-CORDEX/HSIs/'\n",
    "dir_ERA5L_HSIs = f'{path_main}Data/ERA5-Land/HSIs/'\n",
    "dir_names      = f'{path_main}Scripts/Model_lists/'\n",
    "dir_tmp        = f'{path_main}Data/ERA5-Land/'\n",
    "dir_regr       = f'{path_grids}Regridding/'\n",
    "dir_COR_out    = f'{path_main}Data/EURO-CORDEX/HSI_stats/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define cities\n",
    "cities = ['Istanbul', 'Moscow', 'London', 'SaintPetersburg', 'Berlin', 'Madrid', 'Kyiv', 'Rome', 'Paris',\n",
    "          'Bucharest', 'Minsk', 'Vienna', 'Hamburg', 'Warsaw', 'Budapest', 'Barcelona', 'Munich', 'Kharkiv',\n",
    "          'Milan', 'Belgrade', 'Prague', 'NizhnyNovgorod', 'Kazan', 'Sofia', 'Brussels', 'Stockholm', 'Oslo',\n",
    "          'Dublin', 'Lisbon', 'Vilnius', 'Copenhagen', 'Helsinki', 'Athens', 'Amsterdam', 'Riga', 'Zagreb']\n",
    "\n",
    "#Define scenarios and variables\n",
    "scenarios = ['historical', 'rcp85']\n",
    "HSI_names = ['TN', 'TX']\n",
    "\n",
    "method_str = ['mu_model', 'sigma_model', 'mu_ERA5L', 'sigma_ERA5L']\n",
    "\n",
    "N = 5\n",
    "N_str = str(N) + 'x' + str(N)\n",
    "\n",
    "# Load city coordinates\n",
    "fname_coords = dir_scripts + 'City_coordinates.yml'\n",
    "with open(fname_coords, 'r') as file:\n",
    "    city_coords = yaml.safe_load(file)\n",
    "\n",
    "#Define models and RCPs which should be used\n",
    "all_models = []\n",
    "with open(dir_names + 'Models_CORDEX-EUR-11_RCP85.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        all_models.append(eval(line[:-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for selecting NxN grid points around city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_data(data, city, city_coords, N, var_name, data_source):\n",
    "    \n",
    "    #Convert longitude from [0, 360] to [-180, 180]\n",
    "    if 'longitude' in data.coords:  lat_name, lon_name = 'latitude', 'longitude'\n",
    "    elif 'lon' in data.coords:      lat_name, lon_name = 'lat', 'lon'\n",
    "    if data[lon_name].max()>180:\n",
    "        data[lon_name] = data[lon_name].where(data[lon_name]<180, ((data[lon_name] + 180) % 360) - 180)\n",
    "\n",
    "    #Get lat and lon of city\n",
    "    lat_sel, lon_sel = city_coords[city]\n",
    "\n",
    "    #Find grid point closest to city\n",
    "    loc_city = (np.abs(data[lon_name] - lon_sel)) + (np.abs(data[lat_name] - lat_sel))\n",
    "    ind_city = np.unravel_index(np.argmin(loc_city.values), loc_city.shape)\n",
    "\n",
    "    #Select NxN box around grid point\n",
    "    N1 = int(N/2 - 0.5)\n",
    "    N2 = int(N/2 + 0.5)\n",
    "    lat_rng  = slice(ind_city[0] - N1, ind_city[0] + N2)\n",
    "    lon_rng  = slice(ind_city[1] - N1, ind_city[1] + N2)\n",
    "\n",
    "    if 'rlat' in data.dims:   data_city = data.isel(rlat=lat_rng, rlon=lon_rng)\n",
    "    elif 'x' in data.dims:    data_city = data.isel(y=lat_rng, x=lon_rng)\n",
    "    else:                     data_city = data.isel(latitude=lat_rng, longitude=lon_rng)\n",
    "\n",
    "    #Calculate distance from city center\n",
    "    dist = np.sqrt((data_city[lat_name] - lat_sel)**2 + (data_city[lon_name] - lon_sel)**2)                \n",
    "\n",
    "    #Convert K to Â°C\n",
    "    if (var_name in ['TX', 'TN']) and (data_city[var_name].mean()>200):\n",
    "        data_city = data_city - 273.15\n",
    "\n",
    "    return(data_city, dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate mean and SD for EURO-CORDEX and ERA5-Land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_coll = dict()\n",
    "\n",
    "#Initialize data frame for collecting data\n",
    "insert  = np.empty((len(method_str), len(cities))) * np.NaN                \n",
    "data_pd = pd.DataFrame(data=insert, index=method_str, columns=cities)\n",
    "for model in all_models:\n",
    "    for HSI in HSI_names:\n",
    "        data_coll[HSI + '_' + \"_\".join(model)] = data_pd.copy()\n",
    "        \n",
    "#Loop over cities\n",
    "for city in cities:\n",
    "\n",
    "    print(city)\n",
    "    \n",
    "    fnames_ERA5L = [dir_ERA5L_HSIs + file for file in os.listdir(dir_ERA5L_HSIs) if city in file and '15x15' in file]\n",
    "    if len(fnames_ERA5L)!=1:  sys.exit('Number of filenames not correct')\n",
    "\n",
    "    #Loop over models\n",
    "    for i1, model in enumerate(all_models):         \n",
    "\n",
    "        print(\"_\".join(model), end=', ')\n",
    "\n",
    "        #Get file name\n",
    "        dir_data = dir_CORDEX + city + '/historical/'\n",
    "        files_CORDEX = [dir_data + file for file in os.listdir(dir_data) if \"_\".join(model) in file and '5x5' in file]\n",
    "        if len(files_CORDEX)!=1:  sys.exit('Filename is not unique')\n",
    "        \n",
    "        #Regrid ERA5-Land to CORDEX model grid\n",
    "        file_target = files_CORDEX[0]\n",
    "        fname_in   = fnames_ERA5L[0]\n",
    "        fname_regr = dir_tmp + 'ERA5-Land_Europe_1981-2010_regr_'  + \"_\".join(model) + '_' + city + '.nc'\n",
    "        file_grid  = dir_tmp + 'grid_xy' + \"_\".join(model) + '_' + city\n",
    "        if os.path.exists(file_grid):   os.remove(file_grid)\n",
    "        if os.path.exists(fname_regr):  os.remove(fname_regr)\n",
    "        os.system(\"cdo griddes -selvar,TX \" + file_target + \" > \" + file_grid)\n",
    "        os.system(\"cdo remapbil,\" + file_grid + \" \" + fname_in + \" \" + fname_regr)\n",
    "        \n",
    "        #Read regridded ERA5 data\n",
    "        data_city_ERA5Ld = xr.open_dataset(fname_regr, use_cftime=True)\n",
    "\n",
    "        #Loop over scenarios\n",
    "        for scen in scenarios:\n",
    "\n",
    "            #Get file name\n",
    "            dir_data = dir_CORDEX + city + '/' + scen + '/'\n",
    "            files = [file for file in os.listdir(dir_data) if model[0] in file and model[1] in file and model[2] in file]\n",
    "            files = [file for file in files if \"HSIs-\" + N_str in file]\n",
    "            if len(files)!=1:  sys.exit('File is not unique')\n",
    "\n",
    "            #Read CORDEX data\n",
    "            data_read = xr.open_dataset(dir_data + files[0])\n",
    "\n",
    "            #Concatenate historical and future data\n",
    "            if scen=='historical':  data_city_CORDEX = data_read\n",
    "            else:                   data_city_CORDEX = xr.concat((data_city_CORDEX, data_read), dim='time')  \n",
    "\n",
    "        #Get lat and lon names\n",
    "        if 'rlat' in data_city_CORDEX.dims:\n",
    "            lat_name = 'rlat'\n",
    "            lon_name = 'rlon'\n",
    "        elif 'x' in data_city_CORDEX.dims:\n",
    "            lat_name = 'x'\n",
    "            lon_name = 'y'      \n",
    "        else:\n",
    "            sys.exit('Lat and lon names undefined')\n",
    "            \n",
    "        #Check that CORDEX and ERA5-Land coordinates agree\n",
    "        check1 = np.max(np.abs(data_city_ERA5Ld[lat_name].values - data_city_CORDEX[lat_name].values))\n",
    "        check2 = np.max(np.abs(data_city_ERA5Ld[lon_name].values - data_city_CORDEX[lon_name].values))\n",
    "        if (check1>0.0001) or (check2>0.0001):  sys.exit('Coordinates do not agree')\n",
    "\n",
    "        #Re-index data\n",
    "        if (check1!=0) or (check2!=0):\n",
    "            data_city_ERA5Ld = data_city_ERA5Ld.reindex({lat_name: data_city_CORDEX[lat_name], lon_name: data_city_CORDEX[lon_name]}, method='nearest')\n",
    "\n",
    "        #Select data in time period\n",
    "        time_sel = slice('1981', '2010')\n",
    "        data_city_CORDEX = data_city_CORDEX.sel(time=time_sel)\n",
    "        data_city_ERA5Ld = data_city_ERA5Ld.sel(time=time_sel)\n",
    "\n",
    "        #Select data in summer (JJA)\n",
    "        sel_JJA_CORDEX  = (data_city_CORDEX.time.dt.month>=6) & (data_city_CORDEX.time.dt.month<=8)\n",
    "        sel_JJA_ERA5Ld  = (data_city_ERA5Ld.time.dt.month>=6) & (data_city_ERA5Ld.time.dt.month<=8)\n",
    "        data_JJA_CORDEX = data_city_CORDEX.isel(time=sel_JJA_CORDEX)\n",
    "        data_JJA_ERA5Ld = data_city_ERA5Ld.sel(time=sel_JJA_ERA5Ld)\n",
    "        \n",
    "        #Apply ERA5-Land masking to CORDEX and vice-versa\n",
    "        mask1 = ~np.isnan(data_JJA_ERA5Ld.mean('time'))\n",
    "        data_JJA_CORDEX = data_JJA_CORDEX.where(mask1)\n",
    "        mask2 = ~np.isnan(data_JJA_CORDEX.mean('time'))\n",
    "        data_JJA_ERA5Ld = data_JJA_ERA5Ld.where(mask2)\n",
    "        \n",
    "        #Loop over HSIs\n",
    "        for HSI in HSI_names:\n",
    "\n",
    "            #Save in dataframe\n",
    "            data_coll[HSI + '_' + \"_\".join(model)].loc['mu_model', city] = data_JJA_CORDEX[HSI].mean()\n",
    "            data_coll[HSI + '_' + \"_\".join(model)].loc['mu_ERA5L', city] = data_JJA_ERA5Ld[HSI].mean()\n",
    "            data_coll[HSI + '_' + \"_\".join(model)].loc['sigma_model', city] = data_JJA_CORDEX[HSI].std()\n",
    "            data_coll[HSI + '_' + \"_\".join(model)].loc['sigma_ERA5L', city] = data_JJA_ERA5Ld[HSI].std()\n",
    "\n",
    "        #Remove temporary regridded file\n",
    "        os.remove(file_grid)\n",
    "        os.remove(fname_regr)\n",
    "\n",
    "    #Remove temporary time merged file\n",
    "    print('')\n",
    "    \n",
    "#Save data in file   \n",
    "for model in all_models:\n",
    "    for HSI in HSI_names:    \n",
    "        \n",
    "        #Define file name\n",
    "        dir_save  = dir_COR_out + HSI + '/'\n",
    "        fname_out = dir_save + HSI + '_mean_std_JJA_' + \"_\".join(model) + \"_JJA_1981-2010.csv\"\n",
    "        if not os.path.exists(dir_save):  os.mkdir(dir_save)\n",
    "            \n",
    "        #Save data\n",
    "        data_save = data_coll[HSI + '_' + \"_\".join(model)]\n",
    "        data_save.to_csv(fname_out)    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
