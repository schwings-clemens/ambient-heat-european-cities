{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time as t_util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import cftime\n",
    "import xarray as xr\n",
    "\n",
    "#My functions\n",
    "sys.path.insert(0,'../functions/')\n",
    "import functions_HeatWavesCities as fun_HWC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read main paths\n",
    "with open('../path_main.txt', 'r') as file:    path_main  = file.read()\n",
    "with open('../path_EUR-11.txt', 'r') as file:  path_eur11 = file.read()\n",
    "with open('../path_grids.txt', 'r') as file:   dir_grids  = file.read()\n",
    "\n",
    "dir_CORDEX     = path_eur11\n",
    "dir_scripts    = f'{path_main}Scripts/'\n",
    "dir_names      = f'{path_main}Scripts/Model_lists/'\n",
    "dir_orog       = f'{dir_CORDEX}historical/orog/'\n",
    "dir_COR_out    = f'{path_main}Data/EURO-CORDEX/Variables/'\n",
    "if not os.path.exists(dir_COR_out): os.mkdir(dir_COR_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define cities\n",
    "cities = ['Istanbul', 'Moscow', 'London', 'SaintPetersburg', 'Berlin', 'Madrid', 'Kyiv', 'Rome', 'Paris',\n",
    "          'Bucharest', 'Minsk', 'Vienna', 'Hamburg', 'Warsaw', 'Budapest', 'Barcelona', 'Munich', 'Kharkiv',\n",
    "          'Milan', 'Belgrade', 'Prague', 'NizhnyNovgorod', 'Kazan', 'Sofia', 'Brussels', 'Stockholm', 'Oslo',\n",
    "          'Dublin', 'Lisbon', 'Vilnius', 'Copenhagen', 'Helsinki', 'Athens', 'Amsterdam', 'Riga', 'Zagreb']\n",
    "\n",
    "#Define scenarios and variables\n",
    "scenarios = ['historical', 'rcp85']\n",
    "variables = ['tasmin', 'tasmax']\n",
    "\n",
    "# Load city coordinates\n",
    "fname_coords = dir_scripts + 'City_coordinates.yml'\n",
    "with open(fname_coords, 'r') as file:\n",
    "    city_coords = yaml.safe_load(file)\n",
    "\n",
    "#Define models and RCPs which should be used\n",
    "all_models = dict()\n",
    "all_models['rcp26'] = []\n",
    "all_models['rcp85'] = []\n",
    "with open(dir_names + 'Models_CORDEX-EUR-11_RCP26.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        all_models['rcp26'].append(eval(line[:-1]))\n",
    "with open(dir_names + 'Models_CORDEX-EUR-11_RCP85.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        all_models['rcp85'].append(eval(line[:-1]))\n",
    "        \n",
    "#Add models for historical\n",
    "mod_85 = [\"_\".join(model) for model in all_models['rcp85']]\n",
    "mod_26 = [\"_\".join(model) for model in all_models['rcp26']]\n",
    "all_models['historical'] = [model.split('_') for model in sorted(list(set(mod_85).union(set(mod_26))))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare EURO-CORDEX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loop over scenarios\n",
    "for scen in scenarios:\n",
    "\n",
    "    models = all_models[scen]\n",
    "\n",
    "    #Loop over models\n",
    "    for model in models:            \n",
    "            \n",
    "        #Select time limits\n",
    "        if scen=='historical':\n",
    "            time_sel = slice('1980', '2005')\n",
    "        elif scen in ['rcp26', 'rcp85']:\n",
    "            if (model[0]=='MOHC-HadGEM2-ES') and (model[1]=='MOHC-HadREM3-GA7-05'):\n",
    "                time_sel = slice('2005', '2100')\n",
    "            else:\n",
    "                time_sel = slice('2006', '2100')\n",
    "\n",
    "        print(\"_\".join(model))\n",
    "\n",
    "        #Initialize dict to store data\n",
    "        data_coll = dict()\n",
    "            \n",
    "        #Loop over variables\n",
    "        for variab in variables:\n",
    "\n",
    "            print(\" -\" + variab, end='')\n",
    "\n",
    "            #Get file names\n",
    "            dir_files = dir_CORDEX + scen + '/' + variab + '/'\n",
    "            fnames = [dir_files + file for file in os.listdir(dir_files) if model[0] in file and model[1] in file and model[2] in file]\n",
    "            if len(fnames)==0:  sys.exit('No files for ' + variab + ' found for this model.')\n",
    "            \n",
    "            #Sort filenames\n",
    "            fnames = sorted(fnames)\n",
    "\n",
    "            #Delete December 2005 in RCP2.6 and RCP8.5 files for this model combination\n",
    "            if (model[0]=='MOHC-HadGEM2-ES') and (model[1]=='ICTP-RegCM4-6') and scen in ['rcp26', 'rcp85']:\n",
    "                fnames = [file for file in fnames if ('2005' not in file or '2006' not in file)]                \n",
    "\n",
    "            #Merge single files to one large file\n",
    "            t_sta = t_util.time()\n",
    "            file_merge = dir_COR_out + 'CORDEX_merged_' + variab + '_' + scen + \"_\" + \"_\".join(model) + '_gridpoint_tmp.nc'\n",
    "            if os.path.exists(file_merge): os.remove(file_merge)\n",
    "            os.system('cdo mergetime ' + \" \".join(fnames) + \" \" + file_merge)\n",
    "            t_end = t_util.time()\n",
    "            print(\" -- \" + \"{:.1f}\".format(t_end - t_sta), end='')\n",
    "            \n",
    "            #Open data set\n",
    "            with xr.open_dataset(file_merge, use_cftime=True) as ds:\n",
    "                data = ds.load()\n",
    "                ds.close()\n",
    "\n",
    "            #Convert Â°C to K\n",
    "            if (model[0]=='CNRM-CERFACS-CNRM-CM5') and (model[1]=='CNRM-ALADIN53') and variab in ['tasmin', 'tasmax'] and scen in ['rcp26', 'rcp85']:\n",
    "                attrs = data[variab].attrs\n",
    "                data[variab] = data[variab] + 273.15\n",
    "                data[variab].attrs = attrs\n",
    "                \n",
    "            # Convert from hPa to Pa\n",
    "            if model[1]=='CNRM-ALADIN53' and variab=='ps':\n",
    "                data[variab] = 100 * data[variab]\n",
    "                \n",
    "            #Correct wrong x- and y-values for CNRM-ALADIN53\n",
    "            if model[1]=='CNRM-ALADIN53':\n",
    "                data.x.values[107] = 1337.5\n",
    "                data.y.values[107] = 1337.5           \n",
    "\n",
    "            #Convert longitude from [0, 360] to [-180, 180]\n",
    "            if 'longitude' in data.coords:  lat_name, lon_name = 'latitude', 'longitude'\n",
    "            elif 'lon' in data.coords:      lat_name, lon_name = 'lat', 'lon'\n",
    "            if data[lon_name].max()>180:\n",
    "                data[lon_name] = data[lon_name].where(data[lon_name]<180, ((data[lon_name] + 180) % 360) - 180)\n",
    "                \n",
    "            #Loop over cities\n",
    "            for city in cities:\n",
    "\n",
    "                #Get lat and lon of city\n",
    "                lat_sel, lon_sel = city_coords[city]\n",
    "\n",
    "                #Find grid point closest to city\n",
    "                loc_city = (np.abs(data[lon_name] - lon_sel)) + (np.abs(data[lat_name] - lat_sel))\n",
    "                ind_city = np.unravel_index(np.argmin(loc_city.values), loc_city.shape)\n",
    "                if 'rlat' in data.dims:   data_sel = data.isel(rlat=ind_city[0], rlon=ind_city[1])\n",
    "                elif 'x' in data.dims:    data_sel = data.isel(y=ind_city[0], x=ind_city[1])\n",
    "                else: sys.exit('Coordinate names could not be identified')\n",
    "                    \n",
    "                #Save in dict\n",
    "                data_coll[city + '_' + variab] = data_sel.load()\n",
    "            \n",
    "            #Remove temporarily merged file\n",
    "            os.remove(file_merge)\n",
    "            t_end = t_util.time()\n",
    "            print(\", \" + \"{:.1f}\".format(t_end - t_sta))\n",
    "            \n",
    "        #Loop over cities\n",
    "        for city in cities:\n",
    "            \n",
    "            #Define output folder\n",
    "            dir_city = dir_COR_out + city  + '/'\n",
    "            dir_save = dir_city + scen + '/'\n",
    "            if not os.path.exists(dir_city): os.mkdir(dir_city)\n",
    "            if not os.path.exists(dir_save): os.mkdir(dir_save)\n",
    "            \n",
    "            #Loop over variables\n",
    "            for i2, variab in enumerate(variables):        \n",
    "\n",
    "                #Add missing values in December 2099\n",
    "                if (((model[0]=='MOHC-HadGEM2-ES') and (model[1]=='IPSL-WRF381P') and (scen=='rcp85')) or \n",
    "                    ((model[0]=='MOHC-HadGEM2-ES') and (model[1]=='MOHC-HadREM3-GA7-05') and (scen in ['rcp85', 'rcp26'])) or\n",
    "                    ((model[0]=='MOHC-HadGEM2-ES') and (model[1]=='CLMcom-ETH-COSMO-crCLIM-v1-1') and (scen=='rcp85'))):\n",
    "                    \n",
    "                    fname_tmp = dir_COR_out + 'tmp_time.nc'\n",
    "                    data_corr = data_coll[city + '_' + variab]\n",
    "                    \n",
    "                    if (model[0]=='MOHC-HadGEM2-ES') and (model[1]=='IPSL-WRF381P') and (scen=='rcp85'):\n",
    "                        data_full = fun_HWC.add_missing_data(data_corr, fname_tmp, variab, '20991202', '20991231', 30)\n",
    "                    elif (model[0]=='MOHC-HadGEM2-ES') and (model[1]=='MOHC-HadREM3-GA7-05') and (scen=='rcp26'):\n",
    "                        data_full = fun_HWC.add_missing_data(data_corr, fname_tmp, variab, '20991230', '20991230', 1)\n",
    "                    elif (model[0]=='MOHC-HadGEM2-ES') and (model[1]=='MOHC-HadREM3-GA7-05') and (scen=='rcp85'):\n",
    "                        data_full = fun_HWC.add_missing_data(data_corr, fname_tmp, variab, '20991220', '20991230', 11)\n",
    "                    elif (model[0]=='MOHC-HadGEM2-ES') and (model[1]=='CLMcom-ETH-COSMO-crCLIM-v1-1') and (scen=='rcp85'):\n",
    "                        data_full = fun_HWC.add_missing_data(data_corr, fname_tmp, variab, '20990101', '20991230', 360)\n",
    "                        \n",
    "                    #Save corrected data in dict\n",
    "                    data_full = data_full.sortby('time')\n",
    "                    data_coll[city + '_' + variab] = data_full           \n",
    "                    \n",
    "                #Convert to pandas dataframe\n",
    "                data_convert = data_coll[city + '_' + variab].sel(time=time_sel)\n",
    "                data_convert = data_convert[variab].to_pandas().to_frame(name=variab)\n",
    "                \n",
    "                #Put all variables in one dataframe\n",
    "                if i2==0:\n",
    "                    data_out = data_convert\n",
    "                else:\n",
    "                    data_convert.index = data_out.index\n",
    "                    data_out = pd.concat((data_out, data_convert), axis=1)\n",
    "                \n",
    "                #Correct sea level pressure\n",
    "                if variab=='psl':\n",
    "\n",
    "                    #Read orography data and get temperature data\n",
    "                    orog = fun_HWC.get_orog(model, city, city_coords[city], dir_orog)\n",
    "                    Temp = data_out.loc[:, 'tasmax']\n",
    "\n",
    "                    #Correct pressure\n",
    "                    T   = Temp.values\n",
    "                    psl = data_convert['psl'].values\n",
    "                    p_corr = fun_HWC.corr_press(psl, orog, T)\n",
    "\n",
    "                    #Save in data frame and rename psl -> sp\n",
    "                    data_out.loc[:, 'psl'] = p_corr\n",
    "                    data_out = data_out.rename(columns={'psl': 'sp'})\n",
    "                    \n",
    "            #Select data only from 1981 (to fit different time periods for different variables)\n",
    "            if model[0]=='IPSL-IPSL-CM5A-MR' and model[1]=='KNMI-RACMO22E' and scen=='historical':\n",
    "                data_out = data_out[data_out.index.year>=1981]\n",
    "                    \n",
    "            #Create file name\n",
    "            t1 = str(data_out.index[0].year)\n",
    "            t2 = str(data_out.index[-1].year)\n",
    "            fname_out = dir_save + \"Variables_\" + city + \"_\" + \"_\".join(model) + '_' + scen + '_day_' + t1 + \"-\" + t2 + \".csv\"\n",
    "            \n",
    "#             if os.path.exists(fname_out):\n",
    "#                 os.remove(fname_out)\n",
    "            \n",
    "#             data_out.to_csv(fname_out)\n",
    "            \n",
    "            #Check if file already exists\n",
    "            if os.path.exists(fname_out):\n",
    "                \n",
    "                #Read data and add variables\n",
    "                data_read = pd.read_csv(fname_out)\n",
    "                for variab in variables:\n",
    "                    data_read[variab] = data_out[variab].values\n",
    "                \n",
    "                #Save in file\n",
    "                os.remove(fname_out)\n",
    "                data_read.to_csv(fname_out)\n",
    "                \n",
    "            else:\n",
    "\n",
    "                #Save in file\n",
    "                data_out.to_csv(fname_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cftime\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "dir_fig = f'{path_main}Figures/Test_CORDEX_cities/'\n",
    "models_missing = []\n",
    "\n",
    "#Loop over scenarios\n",
    "for scen in scenarios:\n",
    "\n",
    "    models = all_models[scen]\n",
    "    \n",
    "    #Select time limits\n",
    "    if scen=='historical':\n",
    "        time_sel = slice('1980', '2005')\n",
    "    elif scen in ['rcp26', 'rcp85']:\n",
    "        time_sel = slice('2006', '2100')    \n",
    "\n",
    "    #Loop over models\n",
    "    for model in models:\n",
    "\n",
    "        print(\"_\".join(model), end=': ')\n",
    "        \n",
    "        #Create figures\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 8), subplot_kw=dict(projection=ccrs.Robinson()))\n",
    "                \n",
    "        #Add coastlines and borders\n",
    "#         ax.coastlines(resolution='50m', linewidth=0.75, color='#737373', zorder=0)\n",
    "        ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='#bdbdbd', zorder=0)\n",
    "        \n",
    "        #Loop over variables\n",
    "        for variab in variables:\n",
    "\n",
    "            print(variab, end=', ')\n",
    "\n",
    "            #Folder for files\n",
    "            dir_files = dir_CORDEX + scen + '/' + variab + '/'\n",
    "\n",
    "            #Get file names\n",
    "            fnames = [dir_files + file for file in os.listdir(dir_files) if model[0] in file and model[1] in file and model[2] in file]\n",
    "            fnames = sorted(fnames)\n",
    "\n",
    "            #Delete December 2005 in RCP2.6 and RCP8.5 files for this model combination\n",
    "            if (model[0]=='MOHC-HadGEM2-ES') and (model[1]=='ICTP-RegCM4-6') and scen in ['rcp26', 'rcp85']:\n",
    "                fnames = [file for file in fnames if ('2005' not in file or '2006' not in file)]                \n",
    "\n",
    "            for fname in fnames:\n",
    "                \n",
    "                data = xr.open_dataset(fname).isel(time=0)\n",
    "                \n",
    "                #Convert longitude from [0, 360] to [-180, 180]\n",
    "                if data.lon.max()>180:\n",
    "                    data['lon'] = data.lon.where(data.lon<180, ((data.lon + 180) % 360) - 180)\n",
    "\n",
    "                #Loop over cities\n",
    "                for i1, city in enumerate(cities):\n",
    "\n",
    "                    #Set lat and lon of city\n",
    "                    lat_sel, lon_sel = city_coords[city]   \n",
    "\n",
    "                    ax.coastlines()\n",
    "\n",
    "                    loc_city = (np.abs(data.lon - lon_sel)) + (np.abs(data.lat - lat_sel))\n",
    "                    ind_city = np.unravel_index(np.argmin(loc_city), loc_city.shape)\n",
    "                    if 'rlat' in data.dims:\n",
    "                        data_sel = data.isel(rlat=ind_city[0], rlon=ind_city[1])\n",
    "                    elif 'x' in data.dims:\n",
    "                        data_sel = data.isel(y=ind_city[0], x=ind_city[1])\n",
    "\n",
    "                    val2 = loc_city.where(loc_city==np.min(loc_city))\n",
    "                    ax.scatter(data_sel.lon.values, data_sel.lat.values, transform=ccrs.PlateCarree(), color='k')\n",
    "\n",
    "        #Set extent of map\n",
    "        ax.set_extent([-10.5, 46, 33, 70])\n",
    "        \n",
    "        fig.savefig(dir_fig + \"Overview_\" + \"_\".join(model) + '_' + scen + '.png', bbox_inches='tight', dpi=200)\n",
    "        plt.close(fig)\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
