{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time as t_util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import cftime\n",
    "import xarray as xr\n",
    "\n",
    "#My functions\n",
    "sys.path.insert(0,'./../functions/')\n",
    "import functions_HeatWavesCities as fun_HWC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read main paths\n",
    "with open('../path_main.txt', 'r') as file:   path_main  = file.read()\n",
    "with open('../path_CMIP6.txt', 'r') as file:  path_cmip6 = file.read()\n",
    "with open('../path_grids.txt', 'r') as file:  dir_grids  = file.read()\n",
    "    \n",
    "dir_CMIP6   = path_cmip6\n",
    "dir_scripts = f'{path_main}Scripts/'\n",
    "dir_names   = f'{path_main}Scripts/Model_lists/'\n",
    "dir_orog    = f'{dir_grids}Orography/'\n",
    "dir_tmp     = f'{path_main}Data/CMIP6_tmp/'\n",
    "dir_out     = f'{path_main}Data/CMIP6/Variables/'\n",
    "if not os.path.exists(dir_out): os.mkdir(dir_out)\n",
    "if not os.path.exists(dir_tmp): os.mkdir(dir_tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define cities\n",
    "cities = ['Istanbul', 'Moscow', 'London', 'SaintPetersburg', 'Berlin', 'Madrid', 'Kyiv', 'Rome', 'Paris',\n",
    "          'Bucharest', 'Minsk', 'Vienna', 'Hamburg', 'Warsaw', 'Budapest', 'Barcelona', 'Munich', 'Kharkiv',\n",
    "          'Milan', 'Belgrade', 'Prague', 'NizhnyNovgorod', 'Kazan', 'Sofia', 'Brussels', 'Stockholm', 'Oslo',\n",
    "          'Dublin', 'Lisbon', 'Vilnius', 'Copenhagen', 'Helsinki', 'Athens', 'Amsterdam', 'Riga', 'Zagreb']\n",
    "\n",
    "#Define scenarios and variables\n",
    "scenarios = ['historical', 'ssp585']\n",
    "variables   = ['tasmax']\n",
    "variabs_out = ['tasmax']\n",
    "\n",
    "# Load city coordinates\n",
    "fname_coords = dir_scripts + 'City_coordinates.yml'\n",
    "with open(fname_coords, 'r') as file:\n",
    "    city_coords = yaml.safe_load(file)\n",
    "\n",
    "#Define models and SSPs which should be used\n",
    "all_models = dict()\n",
    "all_models['ssp585'] = []\n",
    "with open(dir_names + 'Models_CMIP6_SSP585.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        all_models['ssp585'].append(line[:-1])\n",
    "        \n",
    "#Add models for historical\n",
    "all_models['historical'] = all_models['ssp585']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare CMIP6 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create temporary folder\n",
    "if not os.path.exists(dir_tmp): os.mkdir(dir_tmp)\n",
    "\n",
    "#Loop over scenarios\n",
    "for scen in scenarios:\n",
    "\n",
    "    models = all_models[scen]\n",
    "    \n",
    "    #Select time limits\n",
    "    if scen=='historical':\n",
    "        time_sel = slice('1980', '2014')\n",
    "    elif scen in ['ssp126', 'ssp585']:\n",
    "        time_sel = slice('2015', '2100')    \n",
    "\n",
    "    #Loop over models\n",
    "    for model in models:\n",
    "        \n",
    "        print(model)\n",
    "            \n",
    "        #Initialize dict to store data\n",
    "        data_coll = dict()\n",
    "            \n",
    "        #Loop over variables\n",
    "        for variab in variables:\n",
    "\n",
    "            print(\" -\" + variab, end='')\n",
    "\n",
    "            #Get file names\n",
    "            fnames = [dir_CMIP6 + file for file in os.listdir(dir_CMIP6) if model +'_' in file and variab in file and scen in file]\n",
    "            if model=='EC-Earth3-Veg' and len(fnames)!=1:\n",
    "                fnames = [file for file in fnames if '_gr_' not in file]\n",
    "            if len(fnames)!=1:\n",
    "                sys.exit('Input file not uniquely defined')\n",
    "            else:\n",
    "                fname_in = fnames[0]\n",
    "            \n",
    "            #Select time period for historical data\n",
    "            t_sta = t_util.time()\n",
    "            if scen=='historical':\n",
    "                data = xr.open_dataset(fname_in, use_cftime=True)\n",
    "                data = data.sel(time=time_sel)\n",
    "                fname_tmp = dir_tmp + model + '_' + scen + '_' + variab + '_tmp.nc'\n",
    "                data.to_netcdf(fname_tmp)\n",
    "            else:\n",
    "                fname_tmp = fname_in\n",
    "            \n",
    "            #Load dataset\n",
    "            with xr.open_dataset(fname_tmp, use_cftime=True) as ds:\n",
    "                data = ds.load()\n",
    "                ds.close()\n",
    "\n",
    "            #Loop over cities\n",
    "            for city in cities:\n",
    "\n",
    "                #Get lat and lon of city\n",
    "                lat_sel, lon_sel = city_coords[city]\n",
    "\n",
    "                #Convert longitude from [-180, 180] to [0, 360]\n",
    "                if lon_sel<0: lon_sel = lon_sel + 360\n",
    "                \n",
    "                #Find grid point closest to city\n",
    "                data_sel = data.sel(lat=lat_sel, lon=lon_sel, method='nearest')\n",
    "                    \n",
    "                #Save in dict\n",
    "                data_coll[city + '_' + variab] = data_sel.load()\n",
    "            \n",
    "            #Delete temporary file\n",
    "            if scen=='historical':\n",
    "                os.remove(fname_tmp)\n",
    "            t_end = t_util.time()\n",
    "            print(\", \" + \"{:.1f}\".format(t_end - t_sta))\n",
    "            \n",
    "        #Loop over cities\n",
    "        for city in cities:\n",
    "            \n",
    "            #Define output folder\n",
    "            dir_city = dir_out + city  + '/'\n",
    "            dir_save = dir_city + scen + '/'\n",
    "            if not os.path.exists(dir_city): os.mkdir(dir_city)\n",
    "            if not os.path.exists(dir_save): os.mkdir(dir_save)\n",
    "            \n",
    "            #Loop over variables\n",
    "            for i2, variab in enumerate(variables):          \n",
    "                    \n",
    "                #Convert to pandas dataframe\n",
    "                data_convert = data_coll[city + '_' + variab].sel(time=time_sel)\n",
    "                data_convert = data_convert[variab].to_pandas().to_frame(name=variab)\n",
    "                \n",
    "                #Put all variables in one dataframe\n",
    "                if i2==0:\n",
    "                    data_out = data_convert\n",
    "                else:\n",
    "                    data_out = pd.concat((data_out, data_convert), axis=1)\n",
    "                \n",
    "                #Correct sea level pressure\n",
    "                if variab=='psl':\n",
    "\n",
    "                    #Read orography data and get temperature data\n",
    "                    orog = fun_HWC.get_orog_CMIP6(model, city, city_coords[city], dir_orog)\n",
    "                    Temp = data_out.loc[:, 'tasmax']\n",
    "\n",
    "                    #Correct pressure\n",
    "                    T   = Temp.values\n",
    "                    psl = data_convert['psl'].values\n",
    "                    p_corr = fun_HWC.corr_press(psl, orog, T)\n",
    "\n",
    "                    #Save in data frame and rename psl -> sp\n",
    "                    data_out.loc[:, 'psl'] = p_corr\n",
    "                    data_out = data_out.rename(columns={'psl': 'sp'})\n",
    "                    \n",
    "            #Create file name\n",
    "            t1 = str(data_out.index[0].year)\n",
    "            t2 = str(data_out.index[-1].year)\n",
    "            fname_out = dir_save + \"Variables_\" + city + \"_\" + model + '_' + scen + '_day_' + t1 + \"-\" + t2 + \".csv\"\n",
    "            \n",
    "            #Check if file already exists\n",
    "            if os.path.exists(fname_out):\n",
    "                \n",
    "                #Read data and add variables\n",
    "                data_read = pd.read_csv(fname_out)\n",
    "                for variab in variabs_out:\n",
    "                    data_read[variab] = data_out[variab].values\n",
    "                \n",
    "                #Save in file\n",
    "                os.remove(fname_out)\n",
    "                data_read.to_csv(fname_out)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                #Save in file\n",
    "                data_out.to_csv(fname_out)\n",
    "                \n",
    "#Remove temporary folder\n",
    "os.rmdir(dir_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
