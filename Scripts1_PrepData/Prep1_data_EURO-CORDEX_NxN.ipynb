{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time as t_util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import cftime\n",
    "import xarray as xr\n",
    "\n",
    "#My functions\n",
    "sys.path.insert(0,'../functions/')\n",
    "import functions_HeatWavesCities as fun_HWC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read main paths\n",
    "with open('../path_main.txt', 'r') as file:    path_main  = file.read()\n",
    "with open('../path_EUR-11.txt', 'r') as file:  path_eur11 = file.read()\n",
    "with open('../path_grids.txt', 'r') as file:   dir_grids  = file.read()\n",
    "    \n",
    "dir_CORDEX     = path_eur11\n",
    "dir_scripts    = f'{path_main}Scripts/'\n",
    "dir_names      = f'{path_main}Scripts/Model_lists/'\n",
    "dir_orog       = f'{dir_CORDEX}historical/orog/'\n",
    "dir_COR_out    = f'{path_main}Data/EURO-CORDEX/Variables/'\n",
    "if not os.path.exists(dir_COR_out): os.mkdir(dir_COR_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define cities\n",
    "cities = ['Istanbul', 'Moscow', 'London', 'SaintPetersburg', 'Berlin', 'Madrid', 'Kyiv', 'Rome', 'Paris',\n",
    "          'Bucharest', 'Minsk', 'Vienna', 'Hamburg', 'Warsaw', 'Budapest', 'Barcelona', 'Munich', 'Kharkiv',\n",
    "          'Milan', 'Belgrade', 'Prague', 'NizhnyNovgorod', 'Kazan', 'Sofia', 'Brussels', 'Stockholm', 'Oslo',\n",
    "          'Dublin', 'Lisbon', 'Vilnius', 'Copenhagen', 'Helsinki', 'Athens', 'Amsterdam', 'Riga', 'Zagreb']\n",
    "\n",
    "#Define scenarios and variables\n",
    "scenarios = ['historical', 'rcp85']\n",
    "variables = ['tasmin', 'tasmax']\n",
    "\n",
    "N = 3\n",
    "N_str = str(N) + 'x' + str(N)\n",
    "\n",
    "# Load city coordinates\n",
    "fname_coords = dir_scripts + 'City_coordinates.yml'\n",
    "with open(fname_coords, 'r') as file:\n",
    "    city_coords = yaml.safe_load(file)\n",
    "\n",
    "#Define models and RCPs which should be used\n",
    "all_models = dict()\n",
    "all_models['rcp26'] = []\n",
    "all_models['rcp85'] = []\n",
    "with open(dir_names + 'Models_CORDEX-EUR-11_RCP26.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        all_models['rcp26'].append(eval(line[:-1]))\n",
    "with open(dir_names + 'Models_CORDEX-EUR-11_RCP85.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        all_models['rcp85'].append(eval(line[:-1]))\n",
    "        \n",
    "#Add models for historical\n",
    "mod_85 = [\"_\".join(model) for model in all_models['rcp85']]\n",
    "mod_26 = [\"_\".join(model) for model in all_models['rcp26']]\n",
    "all_models['historical'] = [model.split('_') for model in sorted(list(set(mod_85).union(set(mod_26))))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models['rcp85'] = [ ['CNRM-CERFACS-CNRM-CM5', 'CLMcom-ETH-COSMO-crCLIM-v1-1', 'r1i1p1'],\n",
    "                        ['CNRM-CERFACS-CNRM-CM5', 'ICTP-RegCM4-6', 'r1i1p1'],\n",
    "                        ['CNRM-CERFACS-CNRM-CM5', 'MOHC-HadREM3-GA7-05', 'r1i1p1'],\n",
    "                        ['MPI-M-MPI-ESM-LR', 'IPSL-WRF381P', 'r1i1p1'],\n",
    "                        ['NCC-NorESM1-M', 'ICTP-RegCM4-6', 'r1i1p1']]\n",
    "\n",
    "all_models['historical'] = [ ['CNRM-CERFACS-CNRM-CM5', 'CLMcom-ETH-COSMO-crCLIM-v1-1', 'r1i1p1'],\n",
    "                        ['CNRM-CERFACS-CNRM-CM5', 'ICTP-RegCM4-6', 'r1i1p1'],\n",
    "                        ['CNRM-CERFACS-CNRM-CM5', 'MOHC-HadREM3-GA7-05', 'r1i1p1'],\n",
    "                        ['MPI-M-MPI-ESM-LR', 'IPSL-WRF381P', 'r1i1p1'],\n",
    "                        ['NCC-NorESM1-M', 'ICTP-RegCM4-6', 'r1i1p1']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare EURO-CORDEX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Loop over scenarios\n",
    "for scen in scenarios:\n",
    "\n",
    "    models = all_models[scen]\n",
    "\n",
    "    #Loop over models\n",
    "    for model in models:\n",
    "        \n",
    "        #Select time limits\n",
    "        if scen=='historical':\n",
    "            time_sel = slice('1980', '2005')\n",
    "        elif scen in ['rcp26', 'rcp85']:\n",
    "            if (model[0]=='MOHC-HadGEM2-ES') and (model[1]=='MOHC-HadREM3-GA7-05'):\n",
    "                time_sel = slice('2005', '2100')\n",
    "            else:\n",
    "                time_sel = slice('2006', '2100')\n",
    "\n",
    "        print(\"_\".join(model))\n",
    "        \n",
    "        #Initialize dict to store data\n",
    "        data_coll = dict()\n",
    "            \n",
    "        #Loop over variables\n",
    "        for variab in variables:\n",
    "\n",
    "            print(\" -\" + variab, end='')\n",
    "\n",
    "            #Get file names\n",
    "            dir_files = dir_CORDEX + scen + '/' + variab + '/'\n",
    "            fnames = [dir_files + file for file in os.listdir(dir_files) if model[0] in file and model[1] in file and model[2] in file]\n",
    "            \n",
    "            #Sort filenames\n",
    "            fnames = sorted(fnames)\n",
    "\n",
    "            #Delete December 2005 in RCP2.6 and RCP8.5 files for this model combination\n",
    "            if (model[0]=='MOHC-HadGEM2-ES') and (model[1]=='ICTP-RegCM4-6') and scen in ['rcp26', 'rcp85']:\n",
    "                fnames = [file for file in fnames if ('2005' not in file or '2006' not in file)]                \n",
    "\n",
    "            #Merge single files to one large file\n",
    "            t_sta = t_util.time()\n",
    "            file_merge = dir_COR_out + 'CORDEX_merged_' + variab + '_' + scen + \"_\" + \"_\".join(model) + '_' + N_str + '_tmp.nc'\n",
    "            if os.path.exists(file_merge): os.remove(file_merge)\n",
    "            os.system('cdo mergetime ' + \" \".join(fnames) + \" \" + file_merge)\n",
    "            t_end = t_util.time()\n",
    "            print(\" -- \" + \"{:.1f}\".format(t_end - t_sta), end='')\n",
    "            \n",
    "            #Open data set\n",
    "            with xr.open_dataset(file_merge, use_cftime=True) as ds:\n",
    "                data = ds.load()\n",
    "                ds.close()\n",
    "\n",
    "            #Convert Â°C to K\n",
    "            if (model[0]=='CNRM-CERFACS-CNRM-CM5') and (model[1]=='CNRM-ALADIN53') and variab in ['tasmin', 'tasmax'] and scen in ['rcp26', 'rcp85']:\n",
    "                attrs = data[variab].attrs\n",
    "                data[variab] = data[variab] + 273.15\n",
    "                data[variab].attrs = attrs\n",
    "                \n",
    "            # Convert from hPa to Pa\n",
    "            if model[1]=='CNRM-ALADIN53' and variab=='ps':\n",
    "                data[variab] = 100 * data[variab]\n",
    "                \n",
    "            #Correct wrong x- and y-values for CNRM-ALADIN53\n",
    "            if model[1]=='CNRM-ALADIN53':\n",
    "                data.x.values[107] = 1337.5\n",
    "                data.y.values[107] = 1337.5\n",
    "                \n",
    "            #Convert longitude from [0, 360] to [-180, 180]\n",
    "            if 'longitude' in data.coords:  lat_name, lon_name = 'latitude', 'longitude'\n",
    "            elif 'lon' in data.coords:      lat_name, lon_name = 'lat', 'lon'\n",
    "            if data[lon_name].max()>180:\n",
    "                data[lon_name] = data[lon_name].where(data[lon_name]<180, ((data[lon_name] + 180) % 360) - 180)\n",
    "                \n",
    "            #Loop over cities\n",
    "            for city in cities:\n",
    "\n",
    "                #Get lat and lon of city\n",
    "                lat_sel, lon_sel = city_coords[city]\n",
    "                \n",
    "                #Find grid point closest to city\n",
    "                loc_city = (np.abs(data[lon_name] - lon_sel)) + (np.abs(data[lat_name] - lat_sel))\n",
    "                ind_city = np.unravel_index(np.argmin(loc_city.values), loc_city.shape)\n",
    "                \n",
    "                #Select NxN box around grid point\n",
    "                N1 = int(N/2 - 0.5)\n",
    "                N2 = int(N/2 + 0.5)\n",
    "                lat_rng  = slice(ind_city[0] - N1, ind_city[0] + N2)\n",
    "                lon_rng  = slice(ind_city[1] - N1, ind_city[1] + N2)\n",
    "                if 'rlat' in data.dims:   data_sel = data.isel(rlat=lat_rng, rlon=lon_rng)\n",
    "                elif 'x' in data.dims:    data_sel = data.isel(y=lat_rng, x=lon_rng)\n",
    "                else: sys.exit('Coordinate names could not be identified')\n",
    "\n",
    "                #Save in dict\n",
    "                data_coll[city + '_' + variab] = data_sel.load()\n",
    "            \n",
    "            #Remove temporarily merged file\n",
    "            os.remove(file_merge)\n",
    "            t_end = t_util.time()\n",
    "            print(\", \" + \"{:.1f}\".format(t_end - t_sta))\n",
    "            \n",
    "        #Loop over cities\n",
    "        for city in cities:\n",
    "            \n",
    "            #Define output folder\n",
    "            dir_city = dir_COR_out + city  + '/'\n",
    "            dir_save = dir_city + scen + '/'\n",
    "            if not os.path.exists(dir_city): os.mkdir(dir_city)\n",
    "            if not os.path.exists(dir_save): os.mkdir(dir_save)\n",
    "            \n",
    "            #Loop over variables\n",
    "            for i2, variab in enumerate(variables):        \n",
    "\n",
    "                #Add missing values in December 2099\n",
    "                if (((model[0]=='MOHC-HadGEM2-ES') and (model[1]=='IPSL-WRF381P') and (scen=='rcp85')) or \n",
    "                    ((model[0]=='MOHC-HadGEM2-ES') and (model[1]=='MOHC-HadREM3-GA7-05') and (scen in ['rcp85', 'rcp26'])) or\n",
    "                    ((model[0]=='MOHC-HadGEM2-ES') and (model[1]=='CLMcom-ETH-COSMO-crCLIM-v1-1') and (scen=='rcp85'))):\n",
    "                    \n",
    "                    fname_tmp = dir_COR_out + 'tmp-' + N_str + '_time.nc'\n",
    "                    data_corr = data_coll[city + '_' + variab]\n",
    "                    \n",
    "                    if (model[0]=='MOHC-HadGEM2-ES') and (model[1]=='IPSL-WRF381P') and (scen=='rcp85'):\n",
    "                        data_full = fun_HWC.add_missing_data(data_corr, fname_tmp, variab, '20991202', '20991231', 30)\n",
    "                    elif (model[0]=='MOHC-HadGEM2-ES') and (model[1]=='MOHC-HadREM3-GA7-05') and (scen=='rcp26'):\n",
    "                        data_full = fun_HWC.add_missing_data(data_corr, fname_tmp, variab, '20991230', '20991230', 1)\n",
    "                    elif (model[0]=='MOHC-HadGEM2-ES') and (model[1]=='MOHC-HadREM3-GA7-05') and (scen=='rcp85'):\n",
    "                        data_full = fun_HWC.add_missing_data(data_corr, fname_tmp, variab, '20991220', '20991230', 11)\n",
    "                    elif (model[0]=='MOHC-HadGEM2-ES') and (model[1]=='CLMcom-ETH-COSMO-crCLIM-v1-1') and (scen=='rcp85'):\n",
    "                        data_full = fun_HWC.add_missing_data(data_corr, fname_tmp, variab, '20990101', '20991230', 360)                    \n",
    "                    \n",
    "                    #Save corrected data in dict\n",
    "                    data_full = data_full.sortby('time')\n",
    "                    data_coll[city + '_' + variab] = data_full                   \n",
    "                    \n",
    "                #Convert to pandas dataframe\n",
    "                data_variab = data_coll[city + '_' + variab].sel(time=time_sel)\n",
    "\n",
    "                #Define lat and lon name\n",
    "                if 'rlat' in data_variab.dims:   lat_name, lon_name = 'rlat', 'rlon'\n",
    "                elif 'x' in data_variab.dims:    lat_name, lon_name = 'x', 'y'   \n",
    "                \n",
    "                #Put all variables in one dataframe\n",
    "                if i2==0:\n",
    "                    data_out = data_variab\n",
    "                else:\n",
    "                    data_variab['time'] = data_out.time\n",
    "                    data_out[variab] = data_variab[variab]\n",
    "                \n",
    "                #Correct sea level pressure\n",
    "                if variab=='psl':\n",
    "\n",
    "                    #Read orography data and get temperature data\n",
    "                    orog = fun_HWC.get_orog(model, city, city_coords[city], dir_orog)\n",
    "                    Temp = data_out['tasmax']\n",
    "\n",
    "                    #Correct pressure\n",
    "                    T   = Temp.values\n",
    "                    psl = data_variab['psl'].values\n",
    "                    p_corr = fun_HWC.corr_press(psl, orog, T)\n",
    "\n",
    "                    #Save in data frame and rename psl -> sp\n",
    "                    data_out['psl'] = (('time', lat_name, lon_name), p_corr)\n",
    "                    data_out = data_out.rename({'psl': 'sp'})\n",
    "                    \n",
    "            #Select data only from 1981 (because not all variables have data for 1980)\n",
    "            if model[0]=='IPSL-IPSL-CM5A-MR' and model[1]=='KNMI-RACMO22E' and scen=='historical':\n",
    "                data_out = data_out.sel(time=slice('1981', None))\n",
    "                \n",
    "            #Save data\n",
    "            t1 = str(data_out.time[0].dt.year.values)\n",
    "            t2 = str(data_out.time[-1].dt.year.values)\n",
    "            fname_out = dir_save + \"Variables-\" + N_str + \"_\" + city + \"_\" + \"_\".join(model) + '_' + scen + '_day_' + t1 + \"-\" + t2 + \".nc\"        \n",
    "            if os.path.exists(fname_out): os.remove(fname_out)\n",
    "            data_out.to_netcdf(fname_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
