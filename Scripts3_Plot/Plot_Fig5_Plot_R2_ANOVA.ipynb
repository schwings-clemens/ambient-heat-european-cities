{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time as t_util\n",
    "import numpy as np\n",
    "import cftime\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "City_coordinates.yml\n",
    "City_names_short.yml\n",
    "City_table.xlsx\n",
    "functions\n",
    "Model_lists\n",
    "OLD\n",
    "path_CMIP5.txt\n",
    "path_CMIP6.txt\n",
    "path_EOBS.txt\n",
    "path_ERA5-Land.txt\n",
    "path_EUR-11.txt\n",
    "path_grids.txt\n",
    "path_gwls.txt\n",
    "path_main.txt\n",
    "Scripts1_PrepData\n",
    "Scripts2_PrepPlot\n",
    "Scripts3_Plot\n",
    "Scripts_SUPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read main path\n",
    "with open('../path_main.txt', 'r') as file:    path_main  = file.read()\n",
    "\n",
    "dir_data    = f'{path_main}Data/EURO-CORDEX/ANOVA_input/'\n",
    "dir_scripts = f'{path_main}Scripts/'\n",
    "dir_names   = f'{path_main}Scripts/Model_lists/'\n",
    "dir_fig     = f'{path_main}Figures/Paper_v2/'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define cities\n",
    "cities = ['Lisbon', 'Madrid', 'Barcelona', 'Rome', 'Athens', 'Istanbul', 'Sofia', 'Bucharest', 'Belgrade',\n",
    "          'Zagreb', 'Milan', 'Budapest', 'Munich', 'Vienna', 'Prague', 'Paris', 'Brussels', 'Amsterdam',\n",
    "          'London', 'Dublin', 'Hamburg', 'Copenhagen', 'Berlin', 'Warsaw', 'Kharkiv', 'Kyiv', 'Minsk','Vilnius', \n",
    "          'Riga', 'Moscow', 'NizhnyNovgorod', 'Kazan', 'SaintPetersburg', 'Helsinki', 'Stockholm', 'Oslo']\n",
    "\n",
    "cities = np.array(cities)\n",
    "\n",
    "#Define HSIs\n",
    "HSIs = ['TX', 'TN']\n",
    "\n",
    "# Load city coordinates\n",
    "fname_coords = dir_scripts + 'City_coordinates.yml'\n",
    "with open(fname_coords, 'r') as file:\n",
    "    city_coords = yaml.safe_load(file)\n",
    "    \n",
    "#Define scenarios and variables\n",
    "RCP = 'rcp85'\n",
    "\n",
    "#Define models and RCPs which should be used\n",
    "models = []\n",
    "with open(dir_names + 'Models_CORDEX-EUR-11_RCP85.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        models.append(eval(line[:-1]))\n",
    "\n",
    "models = ['_'.join(model) for model in models]        \n",
    "        \n",
    "#Define warming levels\n",
    "EMT_change = '3.0K' #'1.0K', '2.0K', \n",
    "\n",
    "N_gridcells = ''\n",
    "# N_gridcells = '_3x3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City: lat, lon, elevation, at sea (yes/no)\n",
    "city_geo = pd.DataFrame(columns=['lat', 'lon', 'elev', 'sea'])\n",
    "city_geo.loc['Istanbul']         = [41.013611,  28.955,      40, 1]\n",
    "city_geo.loc['Moscow']           = [55.75,      37.616667,  124, 0]\n",
    "city_geo.loc['London']           = [51.507222,  -0.1275,     14, 0]\n",
    "city_geo.loc['SaintPetersburg']  = [59.95,      30.3,        13, 1]\n",
    "city_geo.loc['Berlin']           = [52.516667,  13.383333,   34, 0]\n",
    "city_geo.loc['Madrid']           = [40.383333,  -3.716667,  667, 0]\n",
    "city_geo.loc['Kyiv']             = [50.45,      30.523333,  168, 0]\n",
    "city_geo.loc['Rome']             = [41.9,       12.5,        14, 0]\n",
    "city_geo.loc['Paris']            = [48.8567,    2.3508,      34, 0]\n",
    "city_geo.loc['Bucharest']        = [44.4325,    26.103889,   70, 0]\n",
    "city_geo.loc['Minsk']            = [53.9,       27.566667,  198, 0]\n",
    "city_geo.loc['Vienna']           = [48.2,       16.366667,  170, 0]\n",
    "city_geo.loc['Hamburg']          = [53.565278,  10.001389,    6, 0]\n",
    "city_geo.loc['Warsaw']           = [52.233333,  21.016667,   93, 0]\n",
    "city_geo.loc['Budapest']         = [47.4925,    19.051389,  102, 0]\n",
    "city_geo.loc['Barcelona']        = [41.383333,  2.183333,    12, 1]\n",
    "city_geo.loc['Munich']           = [48.133333,  11.566667,  520, 0]\n",
    "city_geo.loc['Kharkiv']          = [50.004444,  36.231389,  152, 0]\n",
    "city_geo.loc['Milan']            = [45.466667,  9.183333,   152, 0]\n",
    "city_geo.loc['Belgrade']         = [44.816667,  20.466667,  116, 0]\n",
    "city_geo.loc['Prague']           = [50.083333,  14.416667,  244, 0]\n",
    "city_geo.loc['NizhnyNovgorod']   = [56.326944,  44.0075,     78, 0]\n",
    "city_geo.loc['Kazan']            = [55.790278,  49.134722,  116, 0]\n",
    "city_geo.loc['Sofia']            = [42.7,       23.33,      580, 0]\n",
    "city_geo.loc['Brussels']         = [50.8467,    4.3525,      76, 0]\n",
    "city_geo.loc['Stockholm']        = [59.329444,  18.068611,   15, 1]\n",
    "city_geo.loc['Oslo']             = [59.913889,  10.752222,   12, 1]\n",
    "city_geo.loc['Dublin']           = [53.35,      -6.266667,    8, 1]\n",
    "city_geo.loc['Lisbon']           = [38.725267,  -9.150019,   15, 1]\n",
    "city_geo.loc['Vilnius']          = [54.683333,  25.283333,  124, 0]\n",
    "city_geo.loc['Copenhagen']       = [55.676111,  12.568333,    5, 1]\n",
    "city_geo.loc['Helsinki']         = [60.170833,  24.9375,     25, 1]\n",
    "city_geo.loc['Athens']           = [37.983972,  23.727806,  153, 1]\n",
    "city_geo.loc['Amsterdam']        = [52.366667,  4.9,         -2, 1]\n",
    "city_geo.loc['Riga']             = [56.948889,  24.106389,    8, 1]\n",
    "city_geo.loc['Zagreb']           = [45.816667,  15.983333,  130, 0]\n",
    "\n",
    "#Source for elevation: https://en.wikipedia.org/wiki/List_of_capital_cities_by_elevation and google\n",
    "\n",
    "#Save data in file\n",
    "city_geo.sort_index().to_excel(dir_data + 'location_cities.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate linear regression to get R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Select methods to show in plot\n",
    "methods = ['HSI-changes', 'Threshold-Exceedance', 'HWMId']\n",
    "\n",
    "#Select which time method to use for HSI changes ('JJA_mean', 'Q90', 'Ymax')\n",
    "t_method = 'Ymax'\n",
    "quantile = 'Q50'\n",
    "\n",
    "#Define threshold levels\n",
    "THR_levels = dict()\n",
    "THR_levels['TN'] = 2 # 20 °C\n",
    "THR_levels['TX'] = 3 # 30 °C\n",
    "\n",
    "#Read mean and standard deviation for EMT levels\n",
    "fname_mean = dir_data + 'ANOVA' + N_gridcells + '_' + 'HSIs-mean_rcp85_EMT.nc'\n",
    "fname_std  = dir_data + 'ANOVA' + N_gridcells + '_' + 'HSIs-SD_rcp85_EMT.nc'\n",
    "data_mean  = xr.open_dataset(fname_mean)\n",
    "data_std   = xr.open_dataset(fname_std)\n",
    "\n",
    "#Read mean and standard deviatione of HSIs in reference period\n",
    "fname_ref_mean = dir_data + 'ANOVA' + N_gridcells + '_' + 'HSIs-mean_' + RCP + '_time-periods.nc'\n",
    "fname_ref_std  = dir_data + 'ANOVA' + N_gridcells + '_' + 'HSIs-SD_' + RCP + '_time-periods.nc'\n",
    "data_ref_mean  = xr.open_dataset(fname_ref_mean)\n",
    "data_ref_std   = xr.open_dataset(fname_ref_std)\n",
    "\n",
    "#Create empty arrays\n",
    "R2     = np.zeros((len(HSIs), len(methods), len(models), 15, 2)) * np.NaN\n",
    "R2_adj = np.zeros((len(HSIs), len(methods), len(models), 15, 2)) * np.NaN\n",
    "pvalues = np.zeros((len(HSIs), len(methods), len(models), 5, 2)) * np.NaN\n",
    "\n",
    "#Loop over different sets of explanatory variables\n",
    "for i0, exp_var in enumerate(range(0,2)):\n",
    "\n",
    "    #Loop over HSIs\n",
    "    for i1, HSI in enumerate(HSIs):\n",
    "\n",
    "        THR_level = 'Level' + str(THR_levels[HSI])\n",
    "\n",
    "        methods = dict()\n",
    "        methods['HSI-changes']          = HSI + '_DeltaChange-' + t_method\n",
    "        methods['Threshold-Exceedance'] = HSI + '_ThrExc-' + THR_level\n",
    "        methods['HWMId']                = HSI + '_HWMId-' + quantile\n",
    "\n",
    "        #Loop over methods\n",
    "        for i2, method in enumerate(methods):\n",
    "\n",
    "            #Read data\n",
    "            fname_read = dir_data + 'ANOVA' + N_gridcells + '_' + methods[method] + '_all_models.csv'\n",
    "            data = pd.read_csv(fname_read, index_col=0)\n",
    "\n",
    "            #Loop over models\n",
    "            for i3, model in enumerate(models):\n",
    "\n",
    "                if exp_var==0:\n",
    "\n",
    "                    #Get data (mean, SD, and change)\n",
    "                    x1 = data_ref_mean.sel(time_period='1981-2010', model=model, drop=True)[HSI]\n",
    "                    x2 = data_ref_std.sel(time_period='1981-2010', model=model, drop=True)[HSI]\n",
    "                    x3 = data_mean.sel(EMT_change='3.0K', model=model, drop=True)[HSI] - data_ref_mean.sel(time_period='1981-2010', model=model, drop=True)[HSI]\n",
    "                    x4 = data_std.sel(EMT_change='3.0K', model=model, drop=True)[HSI] - data_ref_std.sel(time_period='1981-2010', model=model, drop=True)[HSI]\n",
    "                    x1 = x1.to_dataframe()[HSI].sort_index().to_frame(name='mean_ref')\n",
    "                    x2 = x2.to_dataframe()[HSI].sort_index().to_frame(name='std_ref')\n",
    "                    x3 = x3.to_dataframe()[HSI].sort_index().to_frame(name='mean_chg')\n",
    "                    x4 = x4.to_dataframe()[HSI].sort_index().to_frame(name='std_chg')\n",
    "\n",
    "                elif exp_var==1:\n",
    "                \n",
    "                    #Get data (mean, SD, and change)\n",
    "                    x1 = city_geo['lat'].sort_index()\n",
    "                    x2 = city_geo['lon'].sort_index()\n",
    "                    x3 = city_geo['elev'].sort_index()\n",
    "                    x4 = city_geo['sea'].sort_index()\n",
    "                    \n",
    "                #Define all combinations of input variables\n",
    "                X = dict()\n",
    "                X[0]  = x1\n",
    "                X[1]  = x2\n",
    "                X[2]  = x3\n",
    "                X[3]  = x4\n",
    "                X[4]  = pd.concat((x1, x2), axis=1)\n",
    "                X[5]  = pd.concat((x1, x3), axis=1)\n",
    "                X[6]  = pd.concat((x1, x4), axis=1)\n",
    "                X[7]  = pd.concat((x2, x3), axis=1)\n",
    "                X[8]  = pd.concat((x2, x4), axis=1)\n",
    "                X[9]  = pd.concat((x3, x4), axis=1)\n",
    "                X[10] = pd.concat((x1, x2, x3), axis=1)\n",
    "                X[11] = pd.concat((x1, x2, x4), axis=1)\n",
    "                X[12] = pd.concat((x1, x3, x4), axis=1)\n",
    "                X[13] = pd.concat((x2, x3, x4), axis=1)\n",
    "                X[14] = pd.concat((x1, x2, x3, x4), axis=1)\n",
    "\n",
    "                #Get data (change, threshold exceedance, HWMId)\n",
    "                y = data.loc[model]\n",
    "                Y = y.to_frame(name='data').sort_index()\n",
    "\n",
    "                #Check that all dataframes are sorted correctly\n",
    "                if not np.all([np.all(x1.index==x2.index), np.all(x1.index==x3.index), np.all(x1.index==x4.index), np.all(x1.index==Y.index)]):\n",
    "                    sys.exit('Order of cities does not agree')\n",
    "\n",
    "                #Loop over all combinations of input variables\n",
    "                for i4 in range(0, 15):\n",
    "\n",
    "                    #Select explanatory variables and add constant\n",
    "                    X_in = X[i4]\n",
    "                    X_in = sm.add_constant(X_in)\n",
    "\n",
    "                    #Fit linear model\n",
    "                    model_SM = sm.OLS(Y, X_in).fit()\n",
    "\n",
    "                    #Save R2 and R2_adj in arrays\n",
    "                    R2[i1, i2, i3, i4, i0]     = model_SM.rsquared\n",
    "                    R2_adj[i1, i2, i3, i4, i0] = model_SM.rsquared_adj\n",
    "\n",
    "                    #Save p-values in array\n",
    "                    if i4==14:\n",
    "                        pvalues[i1, i2, i3, :, i0] = model_SM.pvalues.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate semipartial correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get all possible permutations\n",
    "perms = []\n",
    "for var in range(1, 5):\n",
    "    perms = perms + list(itertools.combinations(range(4), var))\n",
    "\n",
    "#Create array for storing semipartial correlations\n",
    "corr_semi_parr = np.zeros(R2.shape[0:3] + (8, 4, 2,)) * np.NaN\n",
    "\n",
    "#Loop over different sets of explanatory variables\n",
    "for i0, exp_var in enumerate(range(0,2)):\n",
    "\n",
    "    #Loop over all variables\n",
    "    for index in range(0, 4):\n",
    "\n",
    "        print(\"Index \" + str(index) + ':')\n",
    "\n",
    "        #Add R2 of model with only one variable\n",
    "        corr_semi_parr[:, :, :, 0, index, i0] = R2[:, :, :, index, i0]\n",
    "\n",
    "        #Loop over all permutations\n",
    "        i_corr = 1\n",
    "        for i_top, perm in enumerate(perms):\n",
    "\n",
    "            #Check that at least two permutations are included\n",
    "            if (index in perm) and len(perm)>1:\n",
    "\n",
    "                perm = np.array(perm)\n",
    "\n",
    "                #Get index of R2 that should be subtracted\n",
    "                i_low = [ii for ii, perm1 in enumerate(perms) if perm1==tuple(perm[perm!=index])]\n",
    "                if len(i_low)!=1:\n",
    "                    sys.exit('Something went wrong')\n",
    "                else:\n",
    "                    i_low = i_low[0]\n",
    "\n",
    "                #Calculate semipartial correlation and save in array\n",
    "                corr_semi_parr[:, :, :, i_corr, index, i0] = R2[:, :, :, i_top, i0] - R2[:, :, :, i_low, i0]\n",
    "                i_corr = i_corr + 1\n",
    "\n",
    "                print(i_top, end = ' - ')\n",
    "                print(i_low)\n",
    "\n",
    "        print('')\n",
    "    \n",
    "#Dimensions of corr_semi_parr\n",
    "# 0: indicators (e.g, TX, TN)\n",
    "# 1: heat metrics\n",
    "# 2: climate models\n",
    "# 3: regression models for semipartial correlation\n",
    "# 4: explanatory variables\n",
    "# 5: location (1) or climate (2) factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select methods to show in plot\n",
    "methods_out = ['Change', 'Threshold\\nexceedance', 'heat wave magnitude']\n",
    "\n",
    "#Labels\n",
    "labels_clim = lambda HSI: ['$\\overline{\\mathrm{' + HSI + '}}_{\\mathrm{ref}}$', '$\\sigma_{\\mathrm{' + HSI + ',ref}}$', '$\\Delta\\overline{\\mathrm{' + HSI + '}}$', '$\\Delta\\sigma_{\\mathrm{' + HSI + '}}$']\n",
    "labels_loc  = ['Latitude', 'Longitude', 'Elevation', 'Located at sea']\n",
    "\n",
    "#Colors and hatching\n",
    "colors_clim = ['#fee0d2', '#fc9272', '#ef3b2c', '#a50f15']\n",
    "colors_loc  = ['#7fcdbb', '#41b6c4', '#1d91c0', '#225ea8']\n",
    "\n",
    "THR_levels_vals = dict()\n",
    "THR_levels_vals['TN_Level2'] = '20'\n",
    "THR_levels_vals['TX_Level3'] = '30'\n",
    "\n",
    "letters = ['a)', 'b)']\n",
    "\n",
    "#Titles 2\n",
    "titles = dict()\n",
    "titles['HSI-changes'] = lambda HSI, THR_level : HSI + 'x change'\n",
    "titles['Threshold-Exceedance'] = lambda HSI, THR_level: 'Exceedance\\n' + HSI + '>' + THR_levels_vals[HSI + '_' + THR_level] + '°C'\n",
    "titles['HWMId'] = lambda HSI, THR_level : 'HWMId-' + HSI\n",
    "\n",
    "titles2 = dict()\n",
    "titles2[0] = 'Climate factors'\n",
    "titles2[1] = 'Location factors'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Loop over HSIs\n",
    "p_clim = []\n",
    "p_loc  = []\n",
    "for i1, HSI in enumerate(HSIs):\n",
    "    \n",
    "    #Create figure\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 8))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    \n",
    "    #Loop over methods\n",
    "    for i2, method in enumerate(methods):\n",
    "        \n",
    "        ax = axes[i2]\n",
    "        \n",
    "        print('')\n",
    "        print(method)\n",
    "        \n",
    "        #Loop over different sets of explanatory variables\n",
    "        for i0, exp_var in enumerate(range(0,2)):\n",
    "\n",
    "            if exp_var==0:\n",
    "                colors = colors_clim\n",
    "                labels = labels_clim\n",
    "\n",
    "            else:\n",
    "                colors = colors_loc\n",
    "                labels = labels_loc            \n",
    "            \n",
    "            #Get total R2\n",
    "            R2_tot = corr_semi_parr[i1, i2, :, :, :, i0].sum(axis=2)\n",
    "            R2_tot = R2_tot.mean()\n",
    "\n",
    "            #Get total R2 of significant variables only\n",
    "            p_sel = pvalues[i1, i2, :, :, i0]\n",
    "            p_sel = np.sum(p_sel<0.05, axis=0) / np.shape(p_sel)[0]\n",
    "            sel = p_sel[1::]>0.5\n",
    "            R2_sign = corr_semi_parr[i1, i2, :, :, :, i0]\n",
    "            R2_sign = R2_sign[:, :, sel].sum(axis=2).mean()        \n",
    "\n",
    "            print(R2_sign)\n",
    "            \n",
    "            #Plot total R2\n",
    "            p_R2a = ax.bar(5*i0 + 1.5, R2_tot, width=4.25, color='#d9d9d9')\n",
    "            p_R2b = ax.bar(5*i0 + 1.5, R2_sign, width=4.25, color='#bdbdbd')\n",
    "\n",
    "            ##Loop over explanatory variables\n",
    "            S_tot = 0\n",
    "            for index in range(0, 4):\n",
    "\n",
    "                #Get semipartial correlation\n",
    "                semi_sel = corr_semi_parr[i1, i2, :, :, index, i0]\n",
    "\n",
    "                S_mean = semi_sel.mean()\n",
    "\n",
    "                #Get p-values\n",
    "                p_sel = pvalues[i1, i2, :, index + 1, i0]\n",
    "                p_sel = np.sum(p_sel<0.05) / len(p_sel)\n",
    "\n",
    "                #Define hatching based on p-values\n",
    "                if p_sel>0.9:    hatch = 'xxx'\n",
    "                elif p_sel>0.5:  hatch = '///'\n",
    "                else:            hatch = ''\n",
    "\n",
    "                #Get 25th and 75th percentile and create IQR\n",
    "                q25 = np.array(np.quantile(semi_sel, 0.25))\n",
    "                q75 = np.array(np.quantile(semi_sel, 0.75))\n",
    "                yerr = [S_mean - q25, q75 - S_mean]\n",
    "                yerr = np.expand_dims(np.array(yerr), axis=1)\n",
    "\n",
    "\n",
    "                #Plot individual R2 and uncertainty (IQR)\n",
    "                ax.bar(5 * i0 + index, S_mean, color=colors[index], hatch=hatch)\n",
    "                ax.errorbar(5 * i0 + index, S_mean, yerr=yerr, color='k')\n",
    "\n",
    "                if i1==0 and i2==0:\n",
    "                    p_help = ax.bar(200, S_mean, color=colors[index])\n",
    "                    if i0==0:\n",
    "                        p_clim.append(p_help)\n",
    "                    elif i0==1:\n",
    "                        p_loc.append(p_help)\n",
    "\n",
    "                S_tot = S_tot + S_mean  \n",
    "\n",
    "        #Set limits, ticks, and sizes\n",
    "        ax.set_xlim([-1.25, 9.25])\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.set_xticks(np.arange(1.5, 8, 5))\n",
    "        ax.set_xticklabels(['Climate', 'Location'])\n",
    "        ax.set_title(titles[method](HSI, 'Level' + str(THR_levels[HSI])), fontsize=16, pad=10)\n",
    "        if i2!=0:  ax.set_yticks([])\n",
    "        else:      ax.set_ylabel('R$^2$', rotation=0, fontsize=20, labelpad=20)\n",
    "        ax.tick_params(labelsize=16)\n",
    "        ax.tick_params(axis='x', which='major', pad=10)\n",
    "\n",
    "    #Create legends (using dummy scatter plots)\n",
    "    p_help1 = ax.bar(200, 50, color='w', hatch='xxx')\n",
    "    p_help2 = ax.bar(200, 50, color='w', hatch='///')\n",
    "    \n",
    "    #Create legends\n",
    "    leg1 = ax.legend([p_R2a[0], p_R2b[0]], ['Total R$^2$', 'Total R$^2$ (signif. vars)'], fontsize=18, loc=6, bbox_to_anchor=(1, 0.87), ncol=1, frameon=False)\n",
    "    leg2 = ax.legend(p_clim, labels_clim(HSI), fontsize=18, loc=6, bbox_to_anchor=(1, 0.6), ncol=1, frameon=False)\n",
    "    leg3 = ax.legend(p_loc, labels_loc, fontsize=18, loc=6, bbox_to_anchor=(1, 0.3), ncol=1, frameon=False)\n",
    "    leg4 = ax.legend([p_help1, p_help2], ['>90% models signif.', '>50% models signif.'], fontsize=18, loc=6, bbox_to_anchor=(1, 0.06), ncol=1, frameon=False)\n",
    "    ax.add_artist(leg1)\n",
    "    ax.add_artist(leg2)\n",
    "    ax.add_artist(leg3)\n",
    "\n",
    "    #Save figure\n",
    "    if N_gridcells=='' and HSI=='TX':\n",
    "        plt.savefig(dir_fig + 'Fig5_ANOVA' + N_gridcells + '_' + HSI + '.png', bbox_inches='tight', dpi=300)\n",
    "    else:\n",
    "        plt.savefig(dir_fig + 'FigSx_ANOVA' + N_gridcells + '_' + HSI + '.png', bbox_inches='tight', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
