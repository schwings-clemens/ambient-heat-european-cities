{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time as t_util\n",
    "import numpy as np\n",
    "import cftime\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import matplotlib\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read main paths\n",
    "with open('../path_main.txt', 'r') as file:     path_main  = file.read()\n",
    "with open('../path_gwls.txt', 'r') as file:     path_gwls  = file.read()\n",
    "with open('../path_EUR-11.txt', 'r') as file:   path_eur11 = file.read()\n",
    "    \n",
    "dir_CORDEX  = f'{path_main}Data/EURO-CORDEX/HSIs/'\n",
    "dir_GWL     = f'{path_gwls}cmip5_all_ens/'\n",
    "dir_EMT     = f'{path_main}Data/EURO-CORDEX/EMT/'\n",
    "dir_names   = f'{path_main}Scripts/Model_lists/'\n",
    "dir_scripts = f'{path_main}/Scripts/'\n",
    "dir_sftlf   = f'{path_eur11}/historical/sftlf/'\n",
    "dir_out     = f'{path_main}Data/Plot_preparation/HSI_changes/'\n",
    "if not os.path.exists(dir_out): os.mkdir(dir_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define cities\n",
    "cities = ['Lisbon', 'Madrid', 'Barcelona', 'Rome', 'Athens', 'Istanbul', 'Sofia', 'Bucharest', 'Belgrade', 'Zagreb',\n",
    "          'Milan', 'Budapest', 'Munich', 'Vienna', 'Prague', 'Paris', 'Brussels', 'Amsterdam', 'London', 'Dublin',\n",
    "          'Hamburg', 'Copenhagen', 'Berlin', 'Warsaw', 'Kharkiv', 'Kyiv', 'Minsk', 'Vilnius', 'Riga', 'Moscow',\n",
    "          'NizhnyNovgorod', 'Kazan', 'SaintPetersburg', 'Helsinki', 'Stockholm', 'Oslo']\n",
    "\n",
    "# Load city coordinates\n",
    "fname_coords = dir_scripts + 'City_coordinates.yml'\n",
    "with open(fname_coords, 'r') as file:\n",
    "    city_coords = yaml.safe_load(file)\n",
    "\n",
    "#Define HSIs\n",
    "HSI_names = ['TN', 'TX']\n",
    "\n",
    "#Define models and RCPs which should be used\n",
    "all_models = dict()\n",
    "all_models['rcp26'] = []\n",
    "all_models['rcp85'] = []\n",
    "with open(dir_names + 'Models_CORDEX-EUR-11_RCP26.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        all_models['rcp26'].append(eval(line[:-1]))\n",
    "with open(dir_names + 'Models_CORDEX-EUR-11_RCP85.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        all_models['rcp85'].append(eval(line[:-1]))\n",
    "\n",
    "#Read warming levels\n",
    "fname = dir_GWL + 'cmip5_warming_levels_all_ens_1850_1900_no_bounds_check.yml'\n",
    "with open(fname, 'r') as file:\n",
    "    GWL_data = yaml.safe_load(file)\n",
    "\n",
    "#Define warming levels\n",
    "GWL_levels = ['10', '20', '30', '40']\n",
    "\n",
    "#Define out_name\n",
    "out_name = '_3x3'\n",
    "out_name = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read land-sea mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "\n",
    "#Initialize dictionary\n",
    "data_LSM = dict()\n",
    "\n",
    "#Loop over all models\n",
    "for model in all_models['rcp85']:\n",
    "    \n",
    "    #Get file names\n",
    "    fnames = [file for file in os.listdir(dir_sftlf) if model[0] in file and model[1] in file]\n",
    "    \n",
    "    #Define file names for certain models, for which no LSM exists\n",
    "    if model[1]=='IPSL-WRF381P':\n",
    "        fnames = ['sftlf_EUR-11_CNRM-CERFACS-CNRM-CM5_historical_r0i0p0_IPSL-WRF381P_v2-ADAPTED-FROM-OLDER-VERSION_fx.nc']\n",
    "    if model[0]=='ICHEC-EC-EARTH' and model[1]=='CLMcom-ETH-COSMO-crCLIM-v1-1':\n",
    "        fnames = ['sftlf_EUR-11_MPI-M-MPI-ESM-LR_historical_r0i0p0_CLMcom-ETH-COSMO-crCLIM-v1-1_v1_fx.nc']\n",
    "    if model[0]=='IPSL-IPSL-CM5A-MR' and model[1]=='DMI-HIRHAM5':\n",
    "        fnames = ['sftlf_EUR-11_MPI-M-MPI-ESM-LR_historical_r1i1p1_DMI-HIRHAM5_v1_fx.nc']    \n",
    "    \n",
    "    #Get file name\n",
    "    if len(fnames)>=1:\n",
    "        fname = fnames[0]\n",
    "    else:\n",
    "        sys.exit('File name not defined')\n",
    "    \n",
    "    #Read data\n",
    "    data_sftlf = xr.open_dataset(dir_sftlf + fname)\n",
    " \n",
    "    #Convert LSM data to %\n",
    "    if data_sftlf.sftlf.max()<50:\n",
    "        data_sftlf['sftlf'] = 100 * data_sftlf['sftlf']\n",
    "\n",
    "    #Convert longitude from [0, 360] to [-180, 180]\n",
    "    if 'longitude' in data_sftlf.coords:  lat_name, lon_name = 'latitude', 'longitude'\n",
    "    elif 'lon' in data_sftlf.coords:      lat_name, lon_name = 'lat', 'lon'\n",
    "    if data_sftlf[lon_name].max()>180:\n",
    "        data_sftlf[lon_name] = data_sftlf[lon_name].where(data_sftlf[lon_name]<180, ((data_sftlf[lon_name] + 180) % 360) - 180)\n",
    "    \n",
    "    #Loop over cities\n",
    "    for city in cities:\n",
    "        \n",
    "        #Get lat and lon of city\n",
    "        lat_sel, lon_sel = city_coords[city]\n",
    "\n",
    "        #Find grid point closest to city\n",
    "        loc_city = (np.abs(data_sftlf[lon_name] - lon_sel)) + (np.abs(data_sftlf[lat_name] - lat_sel))\n",
    "        ind_city = np.unravel_index(np.argmin(loc_city.values), loc_city.shape)\n",
    "\n",
    "        #Select NxN box around grid point\n",
    "        N1 = int(N/2 - 0.5)\n",
    "        N2 = int(N/2 + 0.5)\n",
    "        lat_rng  = slice(ind_city[0] - N1, ind_city[0] + N2)\n",
    "        lon_rng  = slice(ind_city[1] - N1, ind_city[1] + N2)\n",
    "        if 'rlat' in data_sftlf.dims:   data_sftlf_city = data_sftlf.isel(rlat=lat_rng, rlon=lon_rng)\n",
    "        elif 'x' in data_sftlf.dims:    data_sftlf_city = data_sftlf.isel(y=lat_rng, x=lon_rng)\n",
    "        \n",
    "        #Save in dictionary\n",
    "        data_LSM[model[0] + '_' + model[1] + '_' + city] = data_sftlf_city.sftlf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare EMT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define output folder\n",
    "dir_save = dir_out + 'EURO-CORDEX' + out_name + '/'\n",
    "\n",
    "#Define RCP\n",
    "RCP = 'rcp85'\n",
    "scenarios = ['historical', RCP]\n",
    "\n",
    "#Define changes in EMT relative to 1981-2010\n",
    "EMT_change     = np.array([1.0, 2.0, 3.0])\n",
    "EMT_change_str = ['1.0K', '2.0K', '3.0K']\n",
    "\n",
    "#Define the statistical method\n",
    "time_selections = ['JJA_mean', 'Q90', 'Ymax']\n",
    "\n",
    "#Define minimum fraction of land required on each grid point\n",
    "land_fractions = [0, 25, 50, 75, 100]\n",
    "\n",
    "#Combine model names to one string\n",
    "mod_names = [\"_\".join(model) for model in all_models[RCP]]\n",
    "\n",
    "#Create empty numpy array and define coordinates\n",
    "if out_name=='_3x3':\n",
    "    da_empty = np.zeros((len(all_models[RCP]), len(cities), len(EMT_change), len(time_selections), len(land_fractions), 2)) * np.NaN\n",
    "    coords={'model':        ('model', mod_names),\n",
    "            'city':         ('city', cities),\n",
    "            'EMT_change':   ('EMT_change', EMT_change_str),\n",
    "            't_method':     ('t_method', time_selections),\n",
    "            'land_frac':      ('land_frac', land_fractions),\n",
    "            'range_3x3box': ('range_3x3box', ['min', 'max'])}\n",
    "else:\n",
    "    da_empty = np.zeros((len(all_models[RCP]), len(cities), len(EMT_change), len(time_selections))) * np.NaN\n",
    "    coords={'model':      ('model', mod_names),\n",
    "            'city':       ('city', cities),\n",
    "            'EMT_change': ('EMT_change', EMT_change_str),\n",
    "            't_method':   ('t_method', time_selections)}\n",
    "\n",
    "#Create empty data array for storing SREX-averaged heat indices\n",
    "data_coll = xr.Dataset(coords=coords)\n",
    "for HSI in HSI_names:  data_coll[HSI] = (coords.keys(), da_empty.copy())\n",
    "\n",
    "#Loop over cities\n",
    "for city in cities:\n",
    "    \n",
    "    print(city, end=', ')\n",
    "\n",
    "    #Loop over models\n",
    "    for i1, model in enumerate(all_models[RCP]):\n",
    "\n",
    "        #Get name of CMIP5 driving model\n",
    "        if ('CNRM-CERFAC' in model[0]) or ('CSIRO-QCCCE' in model[0]) or ('MPI-M' in model[0]) or ('NOAA-GFDL' in model[0]):\n",
    "            mod_CMIP5 = '-'.join(model[0].split('-')[2:])\n",
    "        else:\n",
    "            mod_CMIP5 = '-'.join(model[0].split('-')[1:])\n",
    "            \n",
    "        #Loop over scenarios\n",
    "        create = 1\n",
    "        for scen in scenarios:\n",
    "\n",
    "            #Define folder\n",
    "            dir_data = dir_CORDEX + city + '/' + scen + '/'\n",
    "\n",
    "            #Read data\n",
    "            fnames_CORDEX = [file for file in os.listdir(dir_data) if scen + '_' in file and model[0] in file and model[1] in file and model[2] in file]\n",
    "            if out_name=='':  fnames_CORDEX = [file for file in fnames_CORDEX if 'HSIs_' + city in file]\n",
    "            else:             fnames_CORDEX = [file for file in fnames_CORDEX if out_name[1::] in file]\n",
    "            if len(fnames_CORDEX)!=1:  sys.exit('File is not unique')\n",
    "            data_read = xr.open_dataset(dir_data + fnames_CORDEX[0])\n",
    "\n",
    "            #Concatenate data\n",
    "            if create==1:\n",
    "                data_CORDEX = data_read\n",
    "                create = 0\n",
    "            else:\n",
    "                data_CORDEX = xr.concat((data_CORDEX, data_read), dim='time')\n",
    "        \n",
    "        #Read European mean temperature (EMT)\n",
    "        files_EMT = sorted([dir_EMT + file for file in os.listdir(dir_EMT) if \"_\".join(model) in file and 'EMT_' in file])\n",
    "        data_EMT  = xr.concat((xr.open_dataset(file) for file in files_EMT), dim='time')\n",
    "\n",
    "        #Calculate EMT relative to 1981-2010 and calculate 20-year means\n",
    "        dataEMT_ref = data_EMT.sel(time=slice('1981', '2010')).mean('time')\n",
    "        dataEMT_rel = data_EMT - dataEMT_ref\n",
    "        dataEMT_20y = dataEMT_rel.rolling(time=20, center=True).mean()\n",
    "\n",
    "        #Loop over selected EMT levels\n",
    "        for dEMT, dEMT_str in zip(EMT_change, EMT_change_str):\n",
    "\n",
    "            #Identify 20-year period in wich level is reached for first time\n",
    "            ind  = np.where(dataEMT_20y.tas>dEMT)[0][0]\n",
    "            central_year = dataEMT_20y.isel(time=ind).time.dt.year\n",
    "            start_year   = int(central_year - 20 / 2)\n",
    "            end_year     = int(central_year + (20 / 2 - 1))\n",
    "            years_sel    = slice(str(start_year), str(end_year))\n",
    "\n",
    "            if end_year>2099:\n",
    "                print(model)\n",
    "                print(end_year)\n",
    "\n",
    "            #Select data\n",
    "            data_20y = data_CORDEX.sel(time=years_sel)\n",
    "            data_ref = data_CORDEX.sel(time=slice('1981', '2010'))\n",
    "            \n",
    "            #Calculate summer average\n",
    "            JJA_20y = data_20y.isel(time=(data_20y.time.dt.month>=6) & (data_20y.time.dt.month<=8))\n",
    "            JJA_ref = data_ref.isel(time=(data_ref.time.dt.month>=6) & (data_ref.time.dt.month<=8))\n",
    "            \n",
    "            #Calculate yearly maximum\n",
    "            Ymax_20y = data_20y.resample(time='1Y').max()\n",
    "            Ymax_ref = data_ref.resample(time='1Y').max()\n",
    "            \n",
    "            #Calculate 90th percentile (for each month in JJA separately)\n",
    "            create = 1\n",
    "            for month in np.arange(6, 9):\n",
    "\n",
    "                sel_mon_20y = JJA_20y.time.dt.month==month\n",
    "                sel_mon_ref = JJA_ref.time.dt.month==month\n",
    "                sel_dat_20y = JJA_20y.isel(time=sel_mon_20y)\n",
    "                sel_dat_ref = JJA_ref.isel(time=sel_mon_ref)\n",
    "                sel_dat_20y = sel_dat_20y.quantile(0.9, dim='time')\n",
    "                sel_dat_ref = sel_dat_ref.quantile(0.9, dim='time')\n",
    "                \n",
    "                if create==1:\n",
    "                    Q90_20y = sel_dat_20y\n",
    "                    Q90_ref = sel_dat_ref\n",
    "                    \n",
    "                    create = 0\n",
    "                else:\n",
    "                    Q90_20y = xr.concat((Q90_20y, sel_dat_20y), dim='month')\n",
    "                    Q90_ref = xr.concat((Q90_ref, sel_dat_ref), dim='month')\n",
    "                    \n",
    "            #Calculate mean of 90th percentiles\n",
    "            Q90_20y = Q90_20y.mean('month')\n",
    "            Q90_ref = Q90_ref.mean('month')\n",
    "            \n",
    "            #Calculate change\n",
    "            data_chg_JJA  = JJA_20y.mean('time') - JJA_ref.mean('time')\n",
    "            data_chg_Q90  = Q90_20y - Q90_ref\n",
    "            data_chg_Ymax = Ymax_20y.mean('time') - Ymax_ref.mean('time')\n",
    "\n",
    "            if out_name=='_3x3':\n",
    "\n",
    "                #Read land fraction for each model and city\n",
    "                land_frac = data_LSM[model[0] + '_' + model[1] + '_' + city]\n",
    "\n",
    "                #Rename land fraction for MPI-WRF combination\n",
    "                if (model[0]=='MPI-M-MPI-ESM-LR') and (model[1]=='IPSL-WRF381P'):\n",
    "                    land_frac = land_frac.rename({'rlat': 'y', 'rlon': 'x'})                       \n",
    "\n",
    "                #Get coordinate names\n",
    "                if 'longitude' in land_frac.coords:  lat_name1, lon_name1 = 'latitude', 'longitude'\n",
    "                elif 'lon' in land_frac.coords:      lat_name1, lon_name1 = 'lat', 'lon'                        \n",
    "                if 'rlon' in land_frac.coords:       lat_name2, lon_name2 = 'rlat', 'rlon'\n",
    "                elif 'x' in land_frac.coords:        lat_name2, lon_name2 = 'y', 'x'   \n",
    "\n",
    "                #Check if coordinates agree and reindex if they are slightly shifted\n",
    "                check1a = np.sum(np.abs(land_frac[lat_name1].values - data_chg_JJA[lat_name1].values))\n",
    "                check1b = np.sum(np.abs(land_frac[lon_name1].values - data_chg_JJA[lon_name1].values))\n",
    "                check2a = np.sum(np.abs(land_frac[lat_name2].values - data_chg_JJA[lat_name2].values))\n",
    "                check2b = np.sum(np.abs(land_frac[lon_name2].values - data_chg_JJA[lon_name2].values))\n",
    "                if (check1a>0.001) or (check1b>0.001):\n",
    "                    sys.exit('Coordinates between data and LSM do not agree')\n",
    "                if (check2a!=0) or (check2b!=0):\n",
    "                    land_frac[lon_name2] = data_chg_JJA[lon_name2]\n",
    "                    land_frac[lat_name2] = data_chg_JJA[lat_name2]                  \n",
    "\n",
    "                #Loop over different minimum land fractions\n",
    "                for land_min in land_fractions:\n",
    "\n",
    "                    #Apply land-sea mask\n",
    "                    data_chg_JJA_land  = data_chg_JJA.where(land_frac>=land_min)\n",
    "                    data_chg_Q90_land  = data_chg_Q90.where(land_frac>=land_min)\n",
    "                    data_chg_Ymax_land = data_chg_Ymax.where(land_frac>=land_min)\n",
    "\n",
    "                    #Test that final array still has all entries\n",
    "                    if data_chg_JJA_land[HSI].shape!=(3,3):  sys.exit('bla')\n",
    "\n",
    "                    #Loop over HSIs\n",
    "                    for HSI in HSI_names:\n",
    "\n",
    "                        #Put data in array\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": dEMT_str, \"t_method\": \"JJA_mean\", \"land_frac\": land_min, 'range_3x3box': 'min'}] = data_chg_JJA_land[HSI].min()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": dEMT_str, \"t_method\": \"JJA_mean\", \"land_frac\": land_min, 'range_3x3box': 'max'}] = data_chg_JJA_land[HSI].max()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": dEMT_str, \"t_method\": \"Q90\", \"land_frac\": land_min, 'range_3x3box': 'min'}]      = data_chg_Q90_land[HSI].min()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": dEMT_str, \"t_method\": \"Q90\", \"land_frac\": land_min, 'range_3x3box': 'max'}]      = data_chg_Q90_land[HSI].max()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": dEMT_str, \"t_method\": \"Ymax\", \"land_frac\": land_min, 'range_3x3box': 'min'}]     = data_chg_Ymax_land[HSI].min()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": dEMT_str, \"t_method\": \"Ymax\", \"land_frac\": land_min, 'range_3x3box': 'max'}]     = data_chg_Ymax_land[HSI].max()\n",
    "            else:\n",
    "                \n",
    "                #Loop over HSIs\n",
    "                for HSI in HSI_names:\n",
    "                    \n",
    "                    #Put data in array\n",
    "                    data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": dEMT_str, \"t_method\": \"JJA_mean\"}] = data_chg_JJA[HSI]\n",
    "                    data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": dEMT_str, \"t_method\": \"Q90\"}]      = data_chg_Q90[HSI]\n",
    "                    data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": dEMT_str, \"t_method\": \"Ymax\"}]     = data_chg_Ymax[HSI]\n",
    "                    \n",
    "#Save data in file\n",
    "fname_out = dir_save + 'HSIs-changes_' + RCP + '_EMT.nc'\n",
    "if os.path.exists(fname_out): os.remove(fname_out)\n",
    "data_coll.to_netcdf(fname_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GWL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define output folder\n",
    "dir_save = dir_out + 'EURO-CORDEX' + out_name + '/'\n",
    "\n",
    "#Define RCP\n",
    "RCP = 'rcp85'\n",
    "scenarios = ['historical', RCP]\n",
    "\n",
    "#Define the statistical method\n",
    "time_selections = ['JJA_mean', 'Q90', 'Ymax']\n",
    "\n",
    "#Define minimum fraction of land required on each grid point\n",
    "land_fractions = [0, 25, 50, 75, 100]\n",
    "\n",
    "#Combine model names to one string\n",
    "mod_names = [\"_\".join(model) for model in all_models[RCP]]\n",
    "\n",
    "#Create empty numpy array and define coordinates\n",
    "if out_name=='_3x3':\n",
    "    da_empty = np.zeros((len(all_models[RCP]), len(cities), len(GWL_levels), len(time_selections), len(land_fractions), 2)) * np.NaN\n",
    "    coords={'model':        ('model', mod_names),\n",
    "            'city':         ('city', cities),\n",
    "            'GWL_level':    ('GWL_level', GWL_levels),\n",
    "            't_method':     ('t_method', time_selections),\n",
    "            'land_frac':    ('land_frac', land_fractions),\n",
    "            'range_3x3box': ('range_3x3box', ['min', 'max'])}\n",
    "else:\n",
    "    da_empty = np.zeros((len(all_models[RCP]), len(cities), len(GWL_levels), len(time_selections))) * np.NaN\n",
    "    coords={'model':     ('model', mod_names),\n",
    "            'city':      ('city', cities),\n",
    "            'GWL_level': ('GWL_level', GWL_levels),\n",
    "            't_method':  ('t_method', time_selections)}\n",
    "\n",
    "#Create empty data array for storing SREX-averaged heat indices\n",
    "data_coll = xr.Dataset(coords=coords)\n",
    "for HSI in HSI_names:  data_coll[HSI] = (coords.keys(), da_empty.copy())\n",
    "\n",
    "missing_data = dict([ (GWL,set()) for GWL in GWL_levels])\n",
    "    \n",
    "#Loop over cities\n",
    "for city in cities:\n",
    "    \n",
    "    print(city, end=', ')\n",
    "\n",
    "    #Loop over models\n",
    "    for i1, model in enumerate(all_models[RCP]):\n",
    "\n",
    "        #Get name of CMIP5 driving model\n",
    "        if ('CNRM-CERFAC' in model[0]) or ('CSIRO-QCCCE' in model[0]) or ('MPI-M' in model[0]) or ('NOAA-GFDL' in model[0]):\n",
    "            mod_CMIP5 = '-'.join(model[0].split('-')[2:])\n",
    "        else:\n",
    "            mod_CMIP5 = '-'.join(model[0].split('-')[1:])\n",
    "            \n",
    "        #Loop over scenarios\n",
    "        create = 1\n",
    "        for scen in scenarios:\n",
    "\n",
    "            #Define folder\n",
    "            dir_data = dir_CORDEX + city + '/' + scen + '/'\n",
    "\n",
    "            #Read data\n",
    "            fnames_CORDEX = [file for file in os.listdir(dir_data) if scen + '_' in file and model[0] in file and model[1] in file and model[2] in file]\n",
    "            if out_name=='':  fnames_CORDEX = [file for file in fnames_CORDEX if 'HSIs_' + city in file]\n",
    "            else:             fnames_CORDEX = [file for file in fnames_CORDEX if out_name[1::] in file]\n",
    "            if len(fnames_CORDEX)!=1:  sys.exit('File is not unique')\n",
    "            data_read = xr.open_dataset(dir_data + fnames_CORDEX[0])\n",
    "\n",
    "            #Concatenate data\n",
    "            if create==1:\n",
    "                data_CORDEX = data_read\n",
    "                create = 0\n",
    "            else:\n",
    "                data_CORDEX = xr.concat((data_CORDEX, data_read), dim='time')            \n",
    "        \n",
    "        #Read time periods when certain global warming levels (GWL) are reached\n",
    "        time_GWL = dict()\n",
    "        for GWL_level in GWL_levels:\n",
    "            data_level = GWL_data['warming_level_' + GWL_level]\n",
    "            entry_sel = [entry for entry in data_level if entry['model']==mod_CMIP5 and entry['exp']==RCP and entry['ensemble']==model[2]]\n",
    "\n",
    "            if entry_sel==[]:\n",
    "                missing_data[GWL_level] = missing_data[GWL_level].union([model[0]])\n",
    "                continue\n",
    "            \n",
    "            #Select data in time period when GWL is reached\n",
    "            start_year = entry_sel[0]['start_year']\n",
    "            end_year   = entry_sel[0]['end_year']\n",
    "            time_GWL = slice(str(entry_sel[0]['start_year']), str(entry_sel[0]['end_year']))\n",
    "            \n",
    "            #Select data\n",
    "            data_20y = data_CORDEX.sel(time=time_GWL)\n",
    "            data_ref = data_CORDEX.sel(time=slice('1981', '2010'))\n",
    "            \n",
    "            #Calculate summer average\n",
    "            JJA_20y = data_20y.isel(time=(data_20y.time.dt.month>=6) & (data_20y.time.dt.month<=8))\n",
    "            JJA_ref = data_ref.isel(time=(data_ref.time.dt.month>=6) & (data_ref.time.dt.month<=8))\n",
    "            \n",
    "            #Calculate yearly maximum\n",
    "            Ymax_20y = data_20y.resample(time='1Y').max()\n",
    "            Ymax_ref = data_ref.resample(time='1Y').max()\n",
    "            \n",
    "            #Calculate 90th percentile (for each month in JJA separately)\n",
    "            create = 1\n",
    "            for month in np.arange(6, 9):\n",
    "\n",
    "                sel_mon_20y = JJA_20y.time.dt.month==month\n",
    "                sel_mon_ref = JJA_ref.time.dt.month==month\n",
    "                sel_dat_20y = JJA_20y.isel(time=sel_mon_20y)\n",
    "                sel_dat_ref = JJA_ref.isel(time=sel_mon_ref)\n",
    "                sel_dat_20y = sel_dat_20y.quantile(0.9, dim='time')\n",
    "                sel_dat_ref = sel_dat_ref.quantile(0.9, dim='time')\n",
    "                \n",
    "                if create==1:\n",
    "                    Q90_20y = sel_dat_20y\n",
    "                    Q90_ref = sel_dat_ref\n",
    "                    create = 0\n",
    "                else:\n",
    "                    Q90_20y = xr.concat((Q90_20y, sel_dat_20y), dim='month')\n",
    "                    Q90_ref = xr.concat((Q90_ref, sel_dat_ref), dim='month')\n",
    "            \n",
    "            #Calculate mean of 90th percentiles\n",
    "            Q90_20y = Q90_20y.mean('month')\n",
    "            Q90_ref = Q90_ref.mean('month')\n",
    "            \n",
    "            #Calculate change\n",
    "            data_chg_JJA  = JJA_20y.mean('time') - JJA_ref.mean('time')\n",
    "            data_chg_Q90  = Q90_20y - Q90_ref\n",
    "            data_chg_Ymax = Ymax_20y.mean('time') - Ymax_ref.mean('time')\n",
    "\n",
    "            #Put data in array\n",
    "            if out_name=='_3x3':\n",
    "\n",
    "                #Read land fraction for each model and city\n",
    "                land_frac = data_LSM[model[0] + '_' + model[1] + '_' + city]\n",
    "\n",
    "                #Rename land fraction for MPI-WRF combination\n",
    "                if (model[0]=='MPI-M-MPI-ESM-LR') and (model[1]=='IPSL-WRF381P'):\n",
    "                    land_frac = land_frac.rename({'rlat': 'y', 'rlon': 'x'})                       \n",
    "\n",
    "                #Get coordinate names\n",
    "                if 'longitude' in land_frac.coords:  lat_name1, lon_name1 = 'latitude', 'longitude'\n",
    "                elif 'lon' in land_frac.coords:      lat_name1, lon_name1 = 'lat', 'lon'                        \n",
    "                if 'rlon' in land_frac.coords:       lat_name2, lon_name2 = 'rlat', 'rlon'\n",
    "                elif 'x' in land_frac.coords:        lat_name2, lon_name2 = 'y', 'x'   \n",
    "\n",
    "                #Check if coordinates agree and reindex if they are slightly shifted\n",
    "                check1a = np.sum(np.abs(land_frac[lat_name1].values - data_chg_JJA[lat_name1].values))\n",
    "                check1b = np.sum(np.abs(land_frac[lon_name1].values - data_chg_JJA[lon_name1].values))\n",
    "                check2a = np.sum(np.abs(land_frac[lat_name2].values - data_chg_JJA[lat_name2].values))\n",
    "                check2b = np.sum(np.abs(land_frac[lon_name2].values - data_chg_JJA[lon_name2].values))\n",
    "                if (check1a>0.001) or (check1b>0.001):\n",
    "                    sys.exit('Coordinates between data and LSM do not agree')\n",
    "                if (check2a!=0) or (check2b!=0):\n",
    "                    land_frac[lon_name2] = data_chg_JJA[lon_name2]\n",
    "                    land_frac[lat_name2] = data_chg_JJA[lat_name2]                  \n",
    "\n",
    "                #Loop over different minimum land fractions\n",
    "                for land_min in land_fractions:\n",
    "\n",
    "                    #Apply land-sea mask\n",
    "                    data_chg_JJA_land  = data_chg_JJA.where(land_frac>=land_min)\n",
    "                    data_chg_Q90_land  = data_chg_Q90.where(land_frac>=land_min)\n",
    "                    data_chg_Ymax_land = data_chg_Ymax.where(land_frac>=land_min)\n",
    "\n",
    "                    #Test that final array still has all entries\n",
    "                    if data_chg_JJA_land[HSI].shape!=(3,3):  sys.exit('bla')\n",
    "\n",
    "                    #Loop over HSIs\n",
    "                    for HSI in HSI_names:\n",
    "                    \n",
    "                        #Put data in array\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": GWL_level, \"t_method\": \"JJA_mean\", \"land_frac\": land_min, 'range_3x3box': 'min'}] = data_chg_JJA_land[HSI].min()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": GWL_level, \"t_method\": \"JJA_mean\", \"land_frac\": land_min, 'range_3x3box': 'max'}] = data_chg_JJA_land[HSI].max()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": GWL_level, \"t_method\": \"Q90\", \"land_frac\": land_min, 'range_3x3box': 'min'}]      = data_chg_Q90_land[HSI].min()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": GWL_level, \"t_method\": \"Q90\", \"land_frac\": land_min, 'range_3x3box': 'max'}]      = data_chg_Q90_land[HSI].max()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": GWL_level, \"t_method\": \"Ymax\", \"land_frac\": land_min, 'range_3x3box': 'min'}]     = data_chg_Ymax_land[HSI].min()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": GWL_level, \"t_method\": \"Ymax\", \"land_frac\": land_min, 'range_3x3box': 'max'}]     = data_chg_Ymax_land[HSI].max()\n",
    "            else:\n",
    "                \n",
    "                #Loop over HSIs\n",
    "                for HSI in HSI_names:\n",
    "                    \n",
    "                    #Put data in array\n",
    "                    data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": GWL_level, \"t_method\": \"JJA_mean\"}] = data_chg_JJA[HSI]\n",
    "                    data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": GWL_level, \"t_method\": \"Q90\"}]      = data_chg_Q90[HSI]\n",
    "                    data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": GWL_level, \"t_method\": \"Ymax\"}]     = data_chg_Ymax[HSI]\n",
    "\n",
    "#Save data in file\n",
    "fname_out = dir_save + 'HSIs-changes_' + RCP + '_GWL.nc'\n",
    "if os.path.exists(fname_out): os.remove(fname_out)\n",
    "data_coll.to_netcdf(fname_out)    \n",
    "\n",
    "print('Missing data:')\n",
    "print(missing_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare time period data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define output folder\n",
    "dir_save = dir_out + 'EURO-CORDEX' + out_name + '/'\n",
    "\n",
    "#Select time periods\n",
    "time_periods = [[2036, 2065],\n",
    "                [2070, 2099]]\n",
    "\n",
    "#Define time string\n",
    "time_strings = [str(time[0]) + '-' + str(time[1]) for time in time_periods]\n",
    "\n",
    "#Define RCPs\n",
    "RCPs = ['rcp85', 'rcp26']\n",
    "scenarios = ['historical', RCP]\n",
    "\n",
    "#Define the statistical method\n",
    "time_selections = ['JJA_mean', 'Q90', 'Ymax']\n",
    "\n",
    "#Define minimum fraction of land required on each grid point\n",
    "land_fractions = [0, 25, 50, 75, 100]\n",
    "\n",
    "#Combine model names to one string\n",
    "mod_names = [\"_\".join(model) for model in all_models[RCP]]\n",
    "\n",
    "#Create empty numpy array and define coordinates\n",
    "if out_name=='_3x3':\n",
    "    da_empty = np.zeros((len(all_models[RCP]), len(cities), len(time_periods), len(time_selections), len(land_fractions), 2)) * np.NaN\n",
    "    coords={'model':        ('model', mod_names),\n",
    "            'city':         ('city', cities),\n",
    "            'time_period':  ('time_period', time_strings),\n",
    "            't_method':     ('t_method', time_selections),\n",
    "            'land_frac':    ('land_frac', land_fractions),\n",
    "            'range_3x3box': ('range_3x3box', ['min', 'max'])}\n",
    "else:\n",
    "    da_empty = np.zeros((len(all_models[RCP]), len(cities), len(time_periods), len(time_selections))) * np.NaN\n",
    "    coords={'model':       ('model', mod_names),\n",
    "            'city':        ('city', cities),\n",
    "            'time_period': ('time_period', time_strings),\n",
    "            't_method':    ('t_method', time_selections)}\n",
    "\n",
    "#Create empty data array for storing SREX-averaged heat indices\n",
    "data_coll = xr.Dataset(coords=coords)\n",
    "for HSI in HSI_names:  data_coll[HSI] = (coords.keys(), da_empty.copy())\n",
    "\n",
    "#Loop over cities\n",
    "for city in cities:\n",
    "    \n",
    "    print(city, end=', ')\n",
    "\n",
    "    #Loop over models\n",
    "    for i1, model in enumerate(all_models[RCP]):\n",
    "\n",
    "        #Get name of CMIP5 driving model\n",
    "        if ('CNRM-CERFAC' in model[0]) or ('CSIRO-QCCCE' in model[0]) or ('MPI-M' in model[0]) or ('NOAA-GFDL' in model[0]):\n",
    "            mod_CMIP5 = '-'.join(model[0].split('-')[2:])\n",
    "        else:\n",
    "            mod_CMIP5 = '-'.join(model[0].split('-')[1:])\n",
    "            \n",
    "        #Loop over scenarios\n",
    "        create = 1\n",
    "        for scen in scenarios:\n",
    "\n",
    "            #Define folder\n",
    "            dir_data = dir_CORDEX + city + '/' + scen + '/'\n",
    "\n",
    "            #Read data\n",
    "            fnames_CORDEX = [file for file in os.listdir(dir_data) if scen + '_' in file and model[0] in file and model[1] in file and model[2] in file]\n",
    "            if out_name=='':  fnames_CORDEX = [file for file in fnames_CORDEX if 'HSIs_' + city in file]\n",
    "            else:             fnames_CORDEX = [file for file in fnames_CORDEX if out_name[1::] in file]\n",
    "            if len(fnames_CORDEX)!=1:  sys.exit('File is not unique')\n",
    "            data_read = xr.open_dataset(dir_data + fnames_CORDEX[0])\n",
    "\n",
    "            #Concatenate data\n",
    "            if create==1:\n",
    "                data_CORDEX = data_read\n",
    "                create = 0\n",
    "            else:\n",
    "                data_CORDEX = xr.concat((data_CORDEX, data_read), dim='time')            \n",
    "        \n",
    "        #Loop over time periods\n",
    "        for time_period, time_string in zip(time_periods, time_strings):\n",
    "\n",
    "            #Select data in selected time period\n",
    "            time_sel = slice(str(time_period[0]), str(time_period[1]))\n",
    "               \n",
    "            #Select data\n",
    "            data_20y = data_CORDEX.sel(time=years_sel)\n",
    "            data_ref = data_CORDEX.sel(time=slice('1981', '2010'))\n",
    "            \n",
    "            #Calculate summer average\n",
    "            JJA_20y = data_20y.isel(time=(data_20y.time.dt.month>=6) & (data_20y.time.dt.month<=8))\n",
    "            JJA_ref = data_ref.isel(time=(data_ref.time.dt.month>=6) & (data_ref.time.dt.month<=8))\n",
    "            \n",
    "            #Calculate yearly maximum\n",
    "            Ymax_20y = data_20y.resample(time='1Y').max()\n",
    "            Ymax_ref = data_ref.resample(time='1Y').max()\n",
    "            \n",
    "            #Calculate 90th percentile (for each month in JJA separately)\n",
    "            create = 1\n",
    "            for month in np.arange(6, 9):\n",
    "\n",
    "                sel_mon_20y = JJA_20y.time.dt.month==month\n",
    "                sel_mon_ref = JJA_ref.time.dt.month==month\n",
    "                sel_dat_20y = JJA_20y.isel(time=sel_mon_20y)\n",
    "                sel_dat_ref = JJA_ref.isel(time=sel_mon_ref)\n",
    "                sel_dat_20y = sel_dat_20y.quantile(0.9, dim='time')\n",
    "                sel_dat_ref = sel_dat_ref.quantile(0.9, dim='time')\n",
    "                \n",
    "                if create==1:\n",
    "                    Q90_20y = sel_dat_20y\n",
    "                    Q90_ref = sel_dat_ref\n",
    "                    \n",
    "                    create = 0\n",
    "                else:\n",
    "                    Q90_20y = xr.concat((Q90_20y, sel_dat_20y), dim='month')\n",
    "                    Q90_ref = xr.concat((Q90_ref, sel_dat_ref), dim='month')\n",
    "                    \n",
    "            #Calculate mean of 90th percentiles\n",
    "            Q90_20y = Q90_20y.mean('month')\n",
    "            Q90_ref = Q90_ref.mean('month')\n",
    "            \n",
    "            #Calculate change\n",
    "            data_chg_JJA  = JJA_20y.mean('time') - JJA_ref.mean('time')\n",
    "            data_chg_Q90  = Q90_20y - Q90_ref\n",
    "            data_chg_Ymax = Ymax_20y.mean('time') - Ymax_ref.mean('time')\n",
    "\n",
    "            if out_name=='_3x3':\n",
    "\n",
    "                #Read land fraction for each model and city\n",
    "                land_frac = data_LSM[model[0] + '_' + model[1] + '_' + city]\n",
    "\n",
    "                #Rename land fraction for MPI-WRF combination\n",
    "                if (model[0]=='MPI-M-MPI-ESM-LR') and (model[1]=='IPSL-WRF381P'):\n",
    "                    land_frac = land_frac.rename({'rlat': 'y', 'rlon': 'x'})                       \n",
    "\n",
    "                #Get coordinate names\n",
    "                if 'longitude' in land_frac.coords:  lat_name1, lon_name1 = 'latitude', 'longitude'\n",
    "                elif 'lon' in land_frac.coords:      lat_name1, lon_name1 = 'lat', 'lon'                        \n",
    "                if 'rlon' in land_frac.coords:       lat_name2, lon_name2 = 'rlat', 'rlon'\n",
    "                elif 'x' in land_frac.coords:        lat_name2, lon_name2 = 'y', 'x'   \n",
    "\n",
    "                #Check if coordinates agree and reindex if they are slightly shifted\n",
    "                check1a = np.sum(np.abs(land_frac[lat_name1].values - data_chg_JJA[lat_name1].values))\n",
    "                check1b = np.sum(np.abs(land_frac[lon_name1].values - data_chg_JJA[lon_name1].values))\n",
    "                check2a = np.sum(np.abs(land_frac[lat_name2].values - data_chg_JJA[lat_name2].values))\n",
    "                check2b = np.sum(np.abs(land_frac[lon_name2].values - data_chg_JJA[lon_name2].values))\n",
    "                if (check1a>0.001) or (check1b>0.001):\n",
    "                    sys.exit('Coordinates between data and LSM do not agree')\n",
    "                if (check2a!=0) or (check2b!=0):\n",
    "                    land_frac[lon_name2] = data_chg_JJA[lon_name2]\n",
    "                    land_frac[lat_name2] = data_chg_JJA[lat_name2]                  \n",
    "\n",
    "                #Loop over different minimum land fractions\n",
    "                for land_min in land_fractions:\n",
    "\n",
    "                    #Apply land-sea mask\n",
    "                    data_chg_JJA_land  = data_chg_JJA.where(land_frac>=land_min)\n",
    "                    data_chg_Q90_land  = data_chg_Q90.where(land_frac>=land_min)\n",
    "                    data_chg_Ymax_land = data_chg_Ymax.where(land_frac>=land_min)\n",
    "\n",
    "                    #Test that final array still has all entries\n",
    "                    if data_chg_JJA_land[HSI].shape!=(3,3):  sys.exit('bla')\n",
    "\n",
    "                    #Loop over HSIs\n",
    "                    for HSI in HSI_names:\n",
    "\n",
    "                        #Put data in array\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"time_period\": time_string, \"t_method\": \"JJA_mean\", \"land_frac\": land_min, 'range_3x3box': 'min'}] = data_chg_JJA_land[HSI].min()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"time_period\": time_string, \"t_method\": \"JJA_mean\", \"land_frac\": land_min, 'range_3x3box': 'max'}] = data_chg_JJA_land[HSI].max()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"time_period\": time_string, \"t_method\": \"Q90\", \"land_frac\": land_min, 'range_3x3box': 'min'}]      = data_chg_Q90_land[HSI].min()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"time_period\": time_string, \"t_method\": \"Q90\", \"land_frac\": land_min, 'range_3x3box': 'max'}]      = data_chg_Q90_land[HSI].max()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"time_period\": time_string, \"t_method\": \"Ymax\", \"land_frac\": land_min, 'range_3x3box': 'min'}]     = data_chg_Ymax_land[HSI].min()\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"time_period\": time_string, \"t_method\": \"Ymax\", \"land_frac\": land_min, 'range_3x3box': 'max'}]     = data_chg_Ymax_land[HSI].max()\n",
    "            else:\n",
    "                    \n",
    "                #Loop over HSIs\n",
    "                for HSI in HSI_names:\n",
    "                                    \n",
    "                    #Put data in array\n",
    "                    data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"time_period\": time_string, \"t_method\": \"JJA_mean\"}] = data_chg_JJA[HSI]\n",
    "                    data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"time_period\": time_string, \"t_method\": \"Q90\"}]      = data_chg_Q90[HSI]\n",
    "                    data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"time_period\": time_string, \"t_method\": \"Ymax\"}]     = data_chg_Ymax[HSI]\n",
    "\n",
    "#Save data in file\n",
    "fname_out = dir_save + 'HSIs-changes_' + RCP + '_time-periods.nc'\n",
    "if os.path.exists(fname_out): os.remove(fname_out)\n",
    "data_coll.to_netcdf(fname_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
