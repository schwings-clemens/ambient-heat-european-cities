{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time as t_util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cftime\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import matplotlib\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read main paths\n",
    "with open('../path_main.txt', 'r') as file:     path_main  = file.read()\n",
    "with open('../path_gwls.txt', 'r') as file:     path_gwls  = file.read()\n",
    "with open('../path_EUR-11.txt', 'r') as file:   path_eur11 = file.read()\n",
    "    \n",
    "dir_CORDEX   = f'{path_main}Data/EURO-CORDEX/HSIs/'\n",
    "dir_HSIstats = f'{path_main}Data/EURO-CORDEX/HSI_stats/'\n",
    "dir_GWL      = f'{path_gwls}cmip5_all_ens/'\n",
    "dir_EMT      = f'{path_main}Data/EURO-CORDEX/EMT/'\n",
    "dir_names    = f'{path_main}Scripts/Model_lists/'\n",
    "dir_scripts  = f'{path_main}/Scripts/'\n",
    "dir_sftlf    = f'{path_eur11}/historical/sftlf/'\n",
    "dir_out      = f'{path_main}Data/Plot_preparation/Threshold_Exceedance/'\n",
    "if not os.path.exists(dir_out): os.mkdir(dir_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define cities\n",
    "cities = ['Lisbon', 'Madrid', 'Barcelona', 'Rome', 'Athens', 'Istanbul', 'Sofia', 'Bucharest', 'Belgrade', 'Zagreb',\n",
    "          'Milan', 'Budapest', 'Munich', 'Vienna', 'Prague', 'Paris', 'Brussels', 'Amsterdam', 'London', 'Dublin',\n",
    "          'Hamburg', 'Copenhagen', 'Berlin', 'Warsaw', 'Kharkiv', 'Kyiv', 'Minsk', 'Vilnius', 'Riga', 'Moscow',\n",
    "          'NizhnyNovgorod', 'Kazan', 'SaintPetersburg', 'Helsinki', 'Stockholm', 'Oslo']\n",
    "\n",
    "# Load city coordinates\n",
    "fname_coords = dir_scripts + 'City_coordinates.yml'\n",
    "with open(fname_coords, 'r') as file:\n",
    "    city_coords = yaml.safe_load(file)\n",
    "    \n",
    "#Define HSIs\n",
    "HSI_names = ['TN', 'TX']\n",
    "\n",
    "#Define models and RCPs which should be used\n",
    "all_models = dict()\n",
    "all_models['rcp26'] = []\n",
    "all_models['rcp85'] = []\n",
    "with open(dir_names + 'Models_CORDEX-EUR-11_RCP26.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        all_models['rcp26'].append(eval(line[:-1]))\n",
    "with open(dir_names + 'Models_CORDEX-EUR-11_RCP85.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        all_models['rcp85'].append(eval(line[:-1]))\n",
    "\n",
    "#Read warming levels\n",
    "fname = dir_GWL + 'cmip5_warming_levels_all_ens_1850_1900_no_bounds_check.yml'\n",
    "with open(fname, 'r') as file:\n",
    "    GWL_data = yaml.safe_load(file)\n",
    "\n",
    "#Define warming levels\n",
    "GWL_levels = ['1981-2010', '10', '20', '30']\n",
    "\n",
    "#Define out_name\n",
    "out_name = '_3x3'\n",
    "# out_name = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Level 1: warm (caution)\n",
    "#Level 2: hot (extreme caution)\n",
    "#Level 3: very_hot (danger)\n",
    "#Level 4: sweltering (extreme danger)\n",
    "thresholds = {}\n",
    "\n",
    "#Tropical nights (and sensitivity tests)\n",
    "thresholds[\"TN_Level1\"] = 15\n",
    "thresholds[\"TN_Level2\"] = 17\n",
    "thresholds[\"TN_Level3\"] = 20\n",
    "thresholds[\"TN_Level4\"] = 23\n",
    "\n",
    "#TX\n",
    "thresholds[\"TX_Level1\"] = 25\n",
    "thresholds[\"TX_Level2\"] = 27\n",
    "thresholds[\"TX_Level3\"] = 30\n",
    "thresholds[\"TX_Level4\"] = 33\n",
    "\n",
    "#Names for NetCDF file\n",
    "threshold_names = [\"Level1\", \"Level2\", \"Level3\", \"Level4\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read land-sea mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "\n",
    "#Initialize dictionary\n",
    "data_LSM = dict()\n",
    "\n",
    "#Loop over all models\n",
    "for model in all_models['rcp85']:\n",
    "    \n",
    "    #Get file names\n",
    "    fnames = [file for file in os.listdir(dir_sftlf) if model[0] in file and model[1] in file]\n",
    "    \n",
    "    #Define file names for certain models, for which no LSM exists\n",
    "    if model[1]=='IPSL-WRF381P':\n",
    "        fnames = ['sftlf_EUR-11_CNRM-CERFACS-CNRM-CM5_historical_r0i0p0_IPSL-WRF381P_v2-ADAPTED-FROM-OLDER-VERSION_fx.nc']\n",
    "    if model[0]=='ICHEC-EC-EARTH' and model[1]=='CLMcom-ETH-COSMO-crCLIM-v1-1':\n",
    "        fnames = ['sftlf_EUR-11_MPI-M-MPI-ESM-LR_historical_r0i0p0_CLMcom-ETH-COSMO-crCLIM-v1-1_v1_fx.nc']\n",
    "    if model[0]=='IPSL-IPSL-CM5A-MR' and model[1]=='DMI-HIRHAM5':\n",
    "        fnames = ['sftlf_EUR-11_MPI-M-MPI-ESM-LR_historical_r1i1p1_DMI-HIRHAM5_v1_fx.nc']    \n",
    "    \n",
    "    #Get file name\n",
    "    if len(fnames)>=1:\n",
    "        fname = fnames[0]\n",
    "    else:\n",
    "        sys.exit('File name not defined')\n",
    "    \n",
    "    #Read data\n",
    "    data_sftlf = xr.open_dataset(dir_sftlf + fname)\n",
    " \n",
    "    #Convert LSM data to %\n",
    "    if data_sftlf.sftlf.max()<50:\n",
    "        data_sftlf['sftlf'] = 100 * data_sftlf['sftlf']\n",
    "\n",
    "    #Convert longitude from [0, 360] to [-180, 180]\n",
    "    if 'longitude' in data_sftlf.coords:  lat_name, lon_name = 'latitude', 'longitude'\n",
    "    elif 'lon' in data_sftlf.coords:      lat_name, lon_name = 'lat', 'lon'\n",
    "    if data_sftlf[lon_name].max()>180:\n",
    "        data_sftlf[lon_name] = data_sftlf[lon_name].where(data_sftlf[lon_name]<180, ((data_sftlf[lon_name] + 180) % 360) - 180)\n",
    "    \n",
    "    #Loop over cities\n",
    "    for city in cities:\n",
    "        \n",
    "        #Get lat and lon of city\n",
    "        lat_sel, lon_sel = city_coords[city]\n",
    "\n",
    "        #Find grid point closest to city\n",
    "        loc_city = (np.abs(data_sftlf[lon_name] - lon_sel)) + (np.abs(data_sftlf[lat_name] - lat_sel))\n",
    "        ind_city = np.unravel_index(np.argmin(loc_city.values), loc_city.shape)\n",
    "\n",
    "        #Select NxN box around grid point\n",
    "        N1 = int(N/2 - 0.5)\n",
    "        N2 = int(N/2 + 0.5)\n",
    "        lat_rng  = slice(ind_city[0] - N1, ind_city[0] + N2)\n",
    "        lon_rng  = slice(ind_city[1] - N1, ind_city[1] + N2)\n",
    "        if 'rlat' in data_sftlf.dims:   data_sftlf_city = data_sftlf.isel(rlat=lat_rng, rlon=lon_rng)\n",
    "        elif 'x' in data_sftlf.dims:    data_sftlf_city = data_sftlf.isel(y=lat_rng, x=lon_rng)\n",
    "        \n",
    "        #Save in dictionary\n",
    "        data_LSM[model[0] + '_' + model[1] + '_' + city] = data_sftlf_city.sftlf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data according to European warming (EMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define output folder\n",
    "dir_save = dir_out + 'EURO-CORDEX' + out_name + '/'\n",
    "\n",
    "#Define RCP\n",
    "RCP = 'rcp85'\n",
    "scenarios = ['historical', RCP]\n",
    "\n",
    "#Define changes in EMT relative to 1981-2010\n",
    "EMT_change     = np.array([0.0, 1.0, 2.0, 3.0])\n",
    "EMT_change_str = ['1981-2010', '1.0K', '2.0K', '3.0K']\n",
    "\n",
    "#Define threshold levels\n",
    "THR_levels = ['Level1', 'Level2', 'Level3', 'Level4']\n",
    "\n",
    "#Define transformations to apply\n",
    "transformations = ['no_trans', 'trans_mean', 'trans_z']\n",
    "\n",
    "#Define minimum fraction of land required on each grid point\n",
    "land_fractions = [0, 25, 50, 75, 100]\n",
    "\n",
    "#Combine model names to one string\n",
    "mod_names = [\"_\".join(model) for model in all_models[RCP]]\n",
    "\n",
    "#Create empty numpy array and define coordinates\n",
    "if out_name=='_3x3':\n",
    "    da_empty = np.zeros((len(all_models[RCP]), len(cities), len(EMT_change), len(THR_levels), len(transformations), len(land_fractions), 2)) * np.NaN\n",
    "    coords={'model':          ('model', mod_names),\n",
    "            'city':           ('city', cities),\n",
    "            'EMT_change':     ('EMT_change', EMT_change_str),\n",
    "            'THR_level':      ('THR_level', THR_levels),\n",
    "            'transformation': ('transformation', transformations),\n",
    "            'land_frac':      ('land_frac', land_fractions),\n",
    "            'range_3x3box':   ('range_3x3box', ['min', 'max'])}\n",
    "else:\n",
    "    da_empty = np.zeros((len(all_models[RCP]), len(cities), len(EMT_change), len(THR_levels), len(transformations))) * np.NaN\n",
    "    coords={'model':          ('model', mod_names),\n",
    "            'city':           ('city', cities),\n",
    "            'EMT_change':     ('EMT_change', EMT_change_str),\n",
    "            'THR_level':      ('THR_level', THR_levels),\n",
    "            'transformation': ('transformation', transformations)}\n",
    "\n",
    "#Create empty data array for storing SREX-averaged heat indices\n",
    "data_coll = xr.Dataset(coords=coords)\n",
    "for HSI in HSI_names:  data_coll[HSI] = (coords.keys(), da_empty.copy())\n",
    "\n",
    "#Loop over cities\n",
    "for city in cities:\n",
    "    \n",
    "    print(city, end=', ')\n",
    "\n",
    "    #Loop over models\n",
    "    for i1, model in enumerate(all_models[RCP]):\n",
    "\n",
    "        #Get name of CMIP5 driving model\n",
    "        if ('CNRM-CERFAC' in model[0]) or ('CSIRO-QCCCE' in model[0]) or ('MPI-M' in model[0]) or ('NOAA-GFDL' in model[0]):\n",
    "            mod_CMIP5 = '-'.join(model[0].split('-')[2:])\n",
    "        else:\n",
    "            mod_CMIP5 = '-'.join(model[0].split('-')[1:])\n",
    "            \n",
    "        #Loop over scenarios\n",
    "        create = 1\n",
    "        for scen in scenarios:\n",
    "\n",
    "            #Define folder\n",
    "            dir_data = dir_CORDEX + city + '/' + scen + '/'\n",
    "\n",
    "            #Read data\n",
    "            fnames_CORDEX = [file for file in os.listdir(dir_data) if scen + '_' in file and model[0] in file and model[1] in file and model[2] in file]\n",
    "            if out_name=='':  fnames_CORDEX = [file for file in fnames_CORDEX if 'HSIs_' + city in file]\n",
    "            else:             fnames_CORDEX = [file for file in fnames_CORDEX if out_name[1::] in file]\n",
    "            if len(fnames_CORDEX)!=1:  sys.exit('File is not unique')\n",
    "            data_read = xr.open_dataset(dir_data + fnames_CORDEX[0])\n",
    "\n",
    "            #Concatenate data\n",
    "            if create==1:\n",
    "                data_CORDEX = data_read\n",
    "                create = 0\n",
    "            else:\n",
    "                data_CORDEX = xr.concat((data_CORDEX, data_read), dim='time')\n",
    "        \n",
    "        #Read European mean temperature (EMT)\n",
    "        files_EMT = sorted([dir_EMT + file for file in os.listdir(dir_EMT) if \"_\".join(model) in file and 'EMT_' in file])\n",
    "        data_EMT  = xr.concat((xr.open_dataset(file) for file in files_EMT), dim='time')\n",
    "\n",
    "        #Calculate EMT relative to 1981-2010 and calculate 20-year means\n",
    "        dataEMT_ref = data_EMT.sel(time=slice('1981', '2010')).mean('time')\n",
    "        dataEMT_rel = data_EMT - dataEMT_ref\n",
    "        dataEMT_20y = dataEMT_rel.rolling(time=20, center=True).mean()\n",
    "\n",
    "        #Loop over selected EMT levels\n",
    "        for dEMT, dEMT_str in zip(EMT_change[1::], EMT_change_str[1::]):\n",
    "\n",
    "            #Identify 20-year period in wich level is reached for first time\n",
    "            ind  = np.where(dataEMT_20y.tas>dEMT)[0][0]\n",
    "            central_year = dataEMT_20y.isel(time=ind).time.dt.year\n",
    "            start_year   = int(central_year - 20 / 2)\n",
    "            end_year     = int(central_year + (20 / 2 - 1))\n",
    "            years_sel    = slice(str(start_year), str(end_year))\n",
    "\n",
    "            if end_year>2099:\n",
    "                print(model)\n",
    "                print(end_year)\n",
    "\n",
    "            #Get data in 20y-period\n",
    "            data_ref = data_CORDEX.sel(time=slice('1981', '2010'))\n",
    "            data_20y = data_CORDEX.sel(time=years_sel)\n",
    "\n",
    "            #Loop over HSIs\n",
    "            for HSI in HSI_names:\n",
    "\n",
    "                data_trans_ref = dict()\n",
    "                data_trans_20y = dict()\n",
    "                \n",
    "                #Read data for z-tranformation\n",
    "                fname_stats = dir_HSIstats + HSI + '/' + HSI + '_mean_std_JJA_' + \"_\".join(model) + \"_JJA_1981-2010.csv\"\n",
    "                data_mu_sig = pd.read_csv(fname_stats, index_col=0)\n",
    "                \n",
    "                #Extract data\n",
    "                mu_CORDEX  = data_mu_sig.loc['mu_model', city]\n",
    "                sig_CORDEX = data_mu_sig.loc['sigma_model', city]\n",
    "                mu_ERA5L   = data_mu_sig.loc['mu_ERA5L', city] - 273.15 * (data_mu_sig.loc['mu_ERA5L', city]>150)\n",
    "                sig_ERA5L  = data_mu_sig.loc['sigma_ERA5L', city]\n",
    "\n",
    "                #transformation to ERA5 mean (and variability)\n",
    "                z_mod_ref = (data_ref[HSI] - mu_CORDEX) / sig_CORDEX\n",
    "                z_mod_20y = (data_20y[HSI] - mu_CORDEX) / sig_CORDEX\n",
    "                data_trans_ref['trans_mean'] = data_ref[HSI] - mu_CORDEX + mu_ERA5L\n",
    "                data_trans_20y['trans_mean'] = data_20y[HSI] - mu_CORDEX + mu_ERA5L\n",
    "                data_trans_ref['trans_z']    = z_mod_ref * sig_ERA5L + mu_ERA5L\n",
    "                data_trans_20y['trans_z']    = z_mod_20y * sig_ERA5L + mu_ERA5L\n",
    "\n",
    "                #Correct TN and TX\n",
    "                if HSI in ['TN', 'TX']:\n",
    "                    data_trans_ref['no_trans'] = data_ref[HSI] - 273.15\n",
    "                    data_trans_20y['no_trans'] = data_20y[HSI] - 273.15\n",
    "                else:\n",
    "                    data_trans_ref['no_trans'] = data_ref[HSI]\n",
    "                    data_trans_20y['no_trans'] = data_20y[HSI]\n",
    "                    \n",
    "                #Loop over levels'time'\n",
    "                for THR_level in THR_levels:\n",
    "                    \n",
    "                    for transformation in transformations:\n",
    "                    \n",
    "                        #Calculate exceedance\n",
    "                        data_exc_ref = data_trans_ref[transformation]>=thresholds[HSI + \"_\" + THR_level]\n",
    "                        data_exc_20y = data_trans_20y[transformation]>=thresholds[HSI + \"_\" + THR_level]\n",
    "                        data_exc_ref = data_exc_ref.sum('time') / (end_year - start_year + 1)\n",
    "                        data_exc_20y = data_exc_20y.sum('time') / (end_year - start_year + 1)\n",
    "                        \n",
    "                        if out_name=='_3x3':\n",
    "                            \n",
    "                            #Read land fraction for each model and city\n",
    "                            land_frac = data_LSM[model[0] + '_' + model[1] + '_' + city]\n",
    "\n",
    "                            #Rename land fraction for MPI-WRF combination\n",
    "                            if (model[0]=='MPI-M-MPI-ESM-LR') and (model[1]=='IPSL-WRF381P'):\n",
    "                                land_frac = land_frac.rename({'rlat': 'y', 'rlon': 'x'})                       \n",
    "\n",
    "                            #Get coordinate names\n",
    "                            if 'longitude' in land_frac.coords:  lat_name1, lon_name1 = 'latitude', 'longitude'\n",
    "                            elif 'lon' in land_frac.coords:      lat_name1, lon_name1 = 'lat', 'lon'                        \n",
    "                            if 'rlon' in land_frac.coords:       lat_name2, lon_name2 = 'rlat', 'rlon'\n",
    "                            elif 'x' in land_frac.coords:        lat_name2, lon_name2 = 'y', 'x'   \n",
    "\n",
    "                            #Check if coordinates agree and reindex if they are slightly shifted\n",
    "                            check1a = np.sum(np.abs(land_frac[lat_name1].values - data_exc_ref[lat_name1].values))\n",
    "                            check1b = np.sum(np.abs(land_frac[lon_name1].values - data_exc_ref[lon_name1].values))\n",
    "                            check2a = np.sum(np.abs(land_frac[lat_name2].values - data_exc_ref[lat_name2].values))\n",
    "                            check2b = np.sum(np.abs(land_frac[lon_name2].values - data_exc_ref[lon_name2].values))\n",
    "                            if (check1a>0.001) or (check1b>0.001):\n",
    "                                sys.exit('Coordinates between data and LSM do not agree')\n",
    "                            if (check2a!=0) or (check2b!=0):\n",
    "                                land_frac[lon_name2] = data_exc_ref[lon_name2]\n",
    "                                land_frac[lat_name2] = data_exc_ref[lat_name2]                  \n",
    "\n",
    "                            #Loop over different minimum land fractions\n",
    "                            for land_min in land_fractions:\n",
    "\n",
    "                                #Apply land-sea mask\n",
    "                                data_exc_ref_land = data_exc_ref.where(land_frac>=land_min)\n",
    "                                data_exc_20y_land = data_exc_20y.where(land_frac>=land_min)\n",
    "\n",
    "                                #Test that final array still has all entries\n",
    "                                if data_exc_ref_land.shape!=(3,3):  sys.exit('bla')\n",
    "\n",
    "                                #Put data in array\n",
    "                                data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": '1981-2010', \"THR_level\": THR_level, \"transformation\": transformation, \"land_frac\": land_min, 'range_3x3box': 'min'}] = data_exc_ref_land.min()\n",
    "                                data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": '1981-2010', \"THR_level\": THR_level, \"transformation\": transformation, \"land_frac\": land_min, 'range_3x3box': 'max'}] = data_exc_ref_land.max()\n",
    "                                data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": dEMT_str, \"THR_level\": THR_level, \"transformation\": transformation, \"land_frac\": land_min, 'range_3x3box': 'min'}]    = data_exc_20y_land.min()\n",
    "                                data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": dEMT_str, \"THR_level\": THR_level, \"transformation\": transformation, \"land_frac\": land_min, 'range_3x3box': 'max'}]    = data_exc_20y_land.max()\n",
    "                        else:\n",
    "                            \n",
    "                            #Put data in array\n",
    "                            data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": '1981-2010', \"THR_level\": THR_level, \"transformation\": transformation}] = data_exc_ref\n",
    "                            data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"EMT_change\": dEMT_str, \"THR_level\": THR_level, \"transformation\": transformation}]    = data_exc_20y\n",
    "                    \n",
    "#Save data in file\n",
    "fname_out = dir_save + 'HSIs-ThresholdExceedance_' + RCP + '_EMT.nc'\n",
    "if os.path.exists(fname_out): os.remove(fname_out)\n",
    "data_coll.to_netcdf(fname_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GWL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define output folder\n",
    "dir_save = dir_out + 'EURO-CORDEX' + out_name + '/'\n",
    "\n",
    "#Define RCP\n",
    "RCP = 'rcp85'\n",
    "scenarios = ['historical', RCP]\n",
    "\n",
    "THR_levels = ['Level1', 'Level2', 'Level3', 'Level4']\n",
    "\n",
    "#Define minimum fraction of land required on each grid point\n",
    "land_fractions = [0, 25, 50, 75, 100]\n",
    "\n",
    "#Combine model names to one string\n",
    "mod_names = [\"_\".join(model) for model in all_models[RCP]]\n",
    "\n",
    "#Create empty numpy array and define coordinates\n",
    "if out_name=='_3x3':\n",
    "    da_empty = np.zeros((len(all_models[RCP]), len(cities), len(GWL_levels), len(THR_levels), len(land_fractions), 2)) * np.NaN\n",
    "    coords={'model':        ('model', mod_names),\n",
    "            'city':         ('city', cities),\n",
    "            'GWL_level':    ('GWL_level', GWL_levels),\n",
    "            'THR_level':    ('THR_level', THR_levels),\n",
    "            'land_frac':    ('land_frac', land_fractions),\n",
    "            'range_3x3box': ('range_3x3box', ['min', 'max'])}\n",
    "else:\n",
    "    da_empty = np.zeros((len(all_models[RCP]), len(cities), len(GWL_levels), len(THR_levels))) * np.NaN\n",
    "    coords={'model':     ('model', mod_names),\n",
    "            'city':      ('city', cities),\n",
    "            'GWL_level': ('GWL_level', GWL_levels),\n",
    "            'THR_level': ('THR_level', THR_levels)}\n",
    "\n",
    "#Create empty data array for storing SREX-averaged heat indices\n",
    "data_coll = xr.Dataset(coords=coords)\n",
    "for HSI in HSI_names:  data_coll[HSI] = (coords.keys(), da_empty.copy())\n",
    "\n",
    "#Loop over cities\n",
    "for city in cities:\n",
    "    \n",
    "    print(city, end=', ')\n",
    "\n",
    "    #Loop over models\n",
    "    for i1, model in enumerate(all_models[RCP]):\n",
    "\n",
    "        #Get name of CMIP5 driving model\n",
    "        if ('CNRM-CERFAC' in model[0]) or ('CSIRO-QCCCE' in model[0]) or ('MPI-M' in model[0]) or ('NOAA-GFDL' in model[0]):\n",
    "            mod_CMIP5 = '-'.join(model[0].split('-')[2:])\n",
    "        else:\n",
    "            mod_CMIP5 = '-'.join(model[0].split('-')[1:])\n",
    "\n",
    "        #Loop over scenarios\n",
    "        create = 1\n",
    "        for scen in scenarios:\n",
    "\n",
    "            #Define folder\n",
    "            dir_data = dir_CORDEX + city + '/' + scen + '/'\n",
    "\n",
    "            #Read data\n",
    "            fnames_CORDEX = [file for file in os.listdir(dir_data) if scen + '_' in file and model[0] in file and model[1] in file and model[2] in file]\n",
    "            if out_name=='':  fnames_CORDEX = [file for file in fnames_CORDEX if 'HSIs_' + city in file]\n",
    "            else:             fnames_CORDEX = [file for file in fnames_CORDEX if out_name[1::] in file]\n",
    "            if len(fnames_CORDEX)!=1:  sys.exit('File is not unique')\n",
    "            data_read = xr.open_dataset(dir_data + fnames_CORDEX[0])\n",
    "\n",
    "            #Concatenate data\n",
    "            if create==1:\n",
    "                data_CORDEX = data_read\n",
    "                create = 0\n",
    "            else:\n",
    "                data_CORDEX = xr.concat((data_CORDEX, data_read), dim='time')            \n",
    "        \n",
    "        #Read time periods when certain global warming levels (GWL) are reached\n",
    "        time_GWL = dict()\n",
    "        for GWL_level in GWL_levels[1::]:\n",
    "            data_level = GWL_data['warming_level_' + GWL_level]\n",
    "            entry_sel = [entry for entry in data_level if entry['model']==mod_CMIP5 and entry['exp']==RCP and entry['ensemble']==model[2]]\n",
    "\n",
    "            #Select data in time period when GWL is reached\n",
    "            start_year = entry_sel[0]['start_year']\n",
    "            end_year   = entry_sel[0]['end_year']\n",
    "            time_GWL = slice(str(entry_sel[0]['start_year']), str(entry_sel[0]['end_year']))\n",
    "\n",
    "            #Get data in 20y-period\n",
    "            data_ref = data_CORDEX.sel(time=slice('1981', '2010'))\n",
    "            data_20y = data_CORDEX.sel(time=time_GWL)\n",
    "            \n",
    "            #Loop over HSIs\n",
    "            for HSI in HSI_names:\n",
    "                \n",
    "                #Correct TN and TX\n",
    "                if HSI in ['TN', 'TX']:\n",
    "                    data_sel_ref = data_ref[HSI] - 273.15\n",
    "                    data_sel_20y = data_20y[HSI] - 273.15\n",
    "                else:\n",
    "                    data_sel_ref = data_ref[HSI]\n",
    "                    data_sel_20y = data_20y[HSI]\n",
    "                \n",
    "                #Loop over levels\n",
    "                for THR_level in THR_levels:\n",
    "                    \n",
    "                    #Calculate exceedance\n",
    "                    data_exc_ref = data_sel_ref>=thresholds[HSI + \"_\" + THR_level]\n",
    "                    data_exc_20y = data_sel_20y>=thresholds[HSI + \"_\" + THR_level]\n",
    "                    data_exc_ref = data_exc_ref.sum('time') / (end_year - start_year + 1)\n",
    "                    data_exc_20y = data_exc_20y.sum('time') / (end_year - start_year + 1)\n",
    "\n",
    "                    if out_name=='_3x3':\n",
    "                        \n",
    "                        #Read land fraction for each model and city\n",
    "                        land_frac = data_LSM[model[0] + '_' + model[1] + '_' + city]\n",
    "\n",
    "                        #Rename land fraction for MPI-WRF combination\n",
    "                        if (model[0]=='MPI-M-MPI-ESM-LR') and (model[1]=='IPSL-WRF381P'):\n",
    "                            land_frac = land_frac.rename({'rlat': 'y', 'rlon': 'x'})                       \n",
    "\n",
    "                        #Get coordinate names\n",
    "                        if 'longitude' in land_frac.coords:  lat_name1, lon_name1 = 'latitude', 'longitude'\n",
    "                        elif 'lon' in land_frac.coords:      lat_name1, lon_name1 = 'lat', 'lon'                        \n",
    "                        if 'rlon' in land_frac.coords:       lat_name2, lon_name2 = 'rlat', 'rlon'\n",
    "                        elif 'x' in land_frac.coords:        lat_name2, lon_name2 = 'y', 'x'   \n",
    "\n",
    "                        #Check if coordinates agree and reindex if they are slightly shifted\n",
    "                        check1a = np.sum(np.abs(land_frac[lat_name1].values - data_exc_ref[lat_name1].values))\n",
    "                        check1b = np.sum(np.abs(land_frac[lon_name1].values - data_exc_ref[lon_name1].values))\n",
    "                        check2a = np.sum(np.abs(land_frac[lat_name2].values - data_exc_ref[lat_name2].values))\n",
    "                        check2b = np.sum(np.abs(land_frac[lon_name2].values - data_exc_ref[lon_name2].values))\n",
    "                        if (check1a>0.001) or (check1b>0.001):\n",
    "                            sys.exit('Coordinates between data and LSM do not agree')\n",
    "                        if (check2a!=0) or (check2b!=0):\n",
    "                            land_frac[lon_name2] = data_exc_ref[lon_name2]\n",
    "                            land_frac[lat_name2] = data_exc_ref[lat_name2]                  \n",
    "\n",
    "                        #Loop over different minimum land fractions\n",
    "                        for land_min in land_fractions:\n",
    "\n",
    "                            #Apply land-sea mask\n",
    "                            data_exc_ref_land = data_exc_ref.where(land_frac>=land_min)\n",
    "                            data_exc_20y_land = data_exc_20y.where(land_frac>=land_min)\n",
    "\n",
    "                            #Test that final array still has all entries\n",
    "                            if data_exc_ref_land.shape!=(3,3):  sys.exit('bla')\n",
    "                        \n",
    "                            #Put data in array\n",
    "                            data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": '1981-2010', \"THR_level\": THR_level, \"land_frac\": land_min, 'range_3x3box': 'min'}] = data_exc_ref_land.min()\n",
    "                            data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": '1981-2010', \"THR_level\": THR_level, \"land_frac\": land_min, 'range_3x3box': 'max'}] = data_exc_ref_land.max()\n",
    "                            data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": GWL_level, \"THR_level\": THR_level, \"land_frac\": land_min, 'range_3x3box': 'min'}]   = data_exc_20y_land.min()\n",
    "                            data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": GWL_level, \"THR_level\": THR_level, \"land_frac\": land_min, 'range_3x3box': 'max'}]   = data_exc_20y_land.max()\n",
    "                    else:\n",
    "                        \n",
    "                        #Put data in array\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": '1981-2010', \"THR_level\": THR_level}] = data_exc_ref\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"GWL_level\": GWL_level, \"THR_level\": THR_level}]   = data_exc_20y\n",
    "\n",
    "#Save data in file\n",
    "fname_out = dir_save + 'HSIs-ThresholdExceedance_' + RCP + '_GWL.nc'\n",
    "if os.path.exists(fname_out): os.remove(fname_out)\n",
    "data_coll.to_netcdf(fname_out)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare time period data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define output folder\n",
    "dir_save = dir_out + 'EURO-CORDEX' + out_name + '/'\n",
    "\n",
    "#Select time periods\n",
    "time_periods = [[1981, 2010],\n",
    "                [2036, 2065],\n",
    "                [2070, 2099]]\n",
    "\n",
    "#Define time string\n",
    "time_strings = [str(time[0]) + '-' + str(time[1]) for time in time_periods]\n",
    "\n",
    "THR_levels = ['Level1', 'Level2', 'Level3', 'Level4']\n",
    "\n",
    "#Define RCPs\n",
    "RCP = 'rcp85'\n",
    "scenarios = ['historical', RCP]\n",
    "\n",
    "#Define minimum fraction of land required on each grid point\n",
    "land_fractions = [0, 25, 50, 75, 100]\n",
    "\n",
    "#Combine model names to one string\n",
    "mod_names = [\"_\".join(model) for model in all_models[RCP]]\n",
    "\n",
    "\n",
    "#Create empty numpy array and define coordinates\n",
    "if out_name=='_3x3':\n",
    "    da_empty = np.zeros((len(all_models[RCP]), len(cities), len(time_periods), len(THR_levels), len(land_fractions), 2)) * np.NaN\n",
    "    coords={'model':        ('model', mod_names),\n",
    "            'city':         ('city', cities),\n",
    "            'time_period':  ('time_period', time_strings),\n",
    "            'THR_level':    ('THR_level', THR_levels),\n",
    "            'land_frac':    ('land_frac', land_fractions),\n",
    "            'range_3x3box': ('range_3x3box', ['min', 'max'])}\n",
    "else:\n",
    "    da_empty = np.zeros((len(all_models[RCP]), len(cities), len(time_periods), len(THR_levels))) * np.NaN\n",
    "    coords={'model':       ('model', mod_names),\n",
    "            'city':        ('city', cities),\n",
    "            'time_period': ('time_period', time_strings),\n",
    "            'THR_level':   ('THR_level', THR_levels)}\n",
    "\n",
    "#Create empty data array for storing SREX-averaged heat indices\n",
    "data_coll = xr.Dataset(coords=coords)\n",
    "for HSI in HSI_names:  data_coll[HSI] = (coords.keys(), da_empty.copy())\n",
    "\n",
    "#Loop over cities\n",
    "for city in cities:\n",
    "    \n",
    "    print(city, end=', ')\n",
    "\n",
    "    #Loop over models\n",
    "    for i1, model in enumerate(all_models[RCP]):\n",
    "\n",
    "        #Get name of CMIP5 driving model\n",
    "        if ('CNRM-CERFAC' in model[0]) or ('CSIRO-QCCCE' in model[0]) or ('MPI-M' in model[0]) or ('NOAA-GFDL' in model[0]):\n",
    "            mod_CMIP5 = '-'.join(model[0].split('-')[2:])\n",
    "        else:\n",
    "            mod_CMIP5 = '-'.join(model[0].split('-')[1:])\n",
    "            \n",
    "        #Loop over scenarios\n",
    "        create = 1\n",
    "        for scen in scenarios:\n",
    "\n",
    "            #Define folder\n",
    "            dir_data = dir_CORDEX + city + '/' + scen + '/'\n",
    "\n",
    "            #Read data\n",
    "            fnames_CORDEX = [file for file in os.listdir(dir_data) if scen + '_' in file and model[0] in file and model[1] in file and model[2] in file]\n",
    "            if out_name=='':  fnames_CORDEX = [file for file in fnames_CORDEX if 'HSIs_' + city in file]\n",
    "            else:             fnames_CORDEX = [file for file in fnames_CORDEX if out_name[1::] in file]\n",
    "            if len(fnames_CORDEX)!=1:  sys.exit('File is not unique')\n",
    "            data_read = xr.open_dataset(dir_data + fnames_CORDEX[0])\n",
    "\n",
    "            #Concatenate data\n",
    "            if create==1:\n",
    "                data_CORDEX = data_read\n",
    "                create = 0\n",
    "            else:\n",
    "                data_CORDEX = xr.concat((data_CORDEX, data_read), dim='time')            \n",
    "        \n",
    "        #Loop over time periods\n",
    "        for time_period, time_string in zip(time_periods, time_strings):\n",
    "\n",
    "            #Select data in selected time period\n",
    "            time_sel = slice(str(time_period[0]), str(time_period[1]))\n",
    "               \n",
    "            #Get data in 20y-period\n",
    "            data_20y = data_CORDEX.sel(time=time_sel)\n",
    "            \n",
    "            #Loop over HSIs\n",
    "            for HSI in HSI_names:\n",
    "                \n",
    "                #Correct TN and TX\n",
    "                if HSI in ['TN', 'TX']:  data_sel = data_20y[HSI] - 273.15\n",
    "                else:                    data_sel = data_20y[HSI]\n",
    "                \n",
    "                #Loop over levels\n",
    "                for THR_level in THR_levels:\n",
    "                    \n",
    "                    #Calculate exceedance\n",
    "                    data_exc = data_sel>=thresholds[HSI + \"_\" + THR_level]\n",
    "                    data_exc = data_exc.sum('time') / (end_year - start_year + 1)\n",
    "                    \n",
    "                    if out_name=='_3x3':\n",
    "                        \n",
    "                        #Read land fraction for each model and city\n",
    "                        land_frac = data_LSM[model[0] + '_' + model[1] + '_' + city]\n",
    "\n",
    "                        #Rename land fraction for MPI-WRF combination\n",
    "                        if (model[0]=='MPI-M-MPI-ESM-LR') and (model[1]=='IPSL-WRF381P'):\n",
    "                            land_frac = land_frac.rename({'rlat': 'y', 'rlon': 'x'})                       \n",
    "\n",
    "                        #Get coordinate names\n",
    "                        if 'longitude' in land_frac.coords:  lat_name1, lon_name1 = 'latitude', 'longitude'\n",
    "                        elif 'lon' in land_frac.coords:      lat_name1, lon_name1 = 'lat', 'lon'                        \n",
    "                        if 'rlon' in land_frac.coords:       lat_name2, lon_name2 = 'rlat', 'rlon'\n",
    "                        elif 'x' in land_frac.coords:        lat_name2, lon_name2 = 'y', 'x'   \n",
    "\n",
    "                        #Check if coordinates agree and reindex if they are slightly shifted\n",
    "                        check1a = np.sum(np.abs(land_frac[lat_name1].values - data_exc[lat_name1].values))\n",
    "                        check1b = np.sum(np.abs(land_frac[lon_name1].values - data_exc[lon_name1].values))\n",
    "                        check2a = np.sum(np.abs(land_frac[lat_name2].values - data_exc[lat_name2].values))\n",
    "                        check2b = np.sum(np.abs(land_frac[lon_name2].values - data_exc[lon_name2].values))\n",
    "                        if (check1a>0.001) or (check1b>0.001):\n",
    "                            sys.exit('Coordinates between data and LSM do not agree')\n",
    "                        if (check2a!=0) or (check2b!=0):\n",
    "                            land_frac[lon_name2] = data_exc[lon_name2]\n",
    "                            land_frac[lat_name2] = data_exc[lat_name2]                  \n",
    "\n",
    "                        #Loop over different minimum land fractions\n",
    "                        for land_min in land_fractions:\n",
    "\n",
    "                            #Apply land-sea mask\n",
    "                            data_exc_land = data_exc.where(land_frac>=land_min)\n",
    "\n",
    "                            #Test that final array still has all entries\n",
    "                            if data_exc_land.shape!=(3,3):  sys.exit('bla')\n",
    "\n",
    "                            #Put data in array\n",
    "                            data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"time_period\": time_string, \"THR_level\": THR_level, \"land_frac\": land_min, 'range_3x3box': 'min'}] = data_exc_land.min()\n",
    "                            data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"time_period\": time_string, \"THR_level\": THR_level, \"land_frac\": land_min, 'range_3x3box': 'max'}] = data_exc_land.max()\n",
    "                    else:\n",
    "                        \n",
    "                        #Put data in array\n",
    "                        data_coll[HSI].loc[{\"model\": \"_\".join(model), \"city\": city, \"time_period\": time_string, \"THR_level\": THR_level}] = data_exc\n",
    "                        \n",
    "#Save data in file\n",
    "fname_out = dir_save + 'HSIs-ThresholdExceedance_' + RCP + '_time-periods.nc'\n",
    "if os.path.exists(fname_out): os.remove(fname_out)\n",
    "data_coll.to_netcdf(fname_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
